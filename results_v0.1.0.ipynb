{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b3df49-3a66-4333-a425-f6b533d49120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import torch\n",
    "from typing import List, Dict, Tuple\n",
    "from datasets import load_dataset, Dataset\n",
    "from dotenv import load_dotenv\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "pipe = pipeline(\"token-classification\", model=\"obi/deid_bert_i2b2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66c6a98f-4885-49df-b6c8-7bcbc9689c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mks-logic/SPY\", trust_remote_code=True, faker_random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fd0d5e2-a112-4d1c-9e6d-6b91c1fe78db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'legal_questions': ['tokens', 'trailing_whitespaces', 'labels', 'ent_tags'], 'medical_consultations': ['tokens', 'trailing_whitespaces', 'labels', 'ent_tags']}\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'bool'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "for x in dataset['medical_consultations']:\n",
    "    # print(x)\n",
    "    break\n",
    "print(dataset.column_names)\n",
    "lst_of_ent_tags = dataset['medical_consultations']['ent_tags']\n",
    "lst_of_tokens = dataset['medical_consultations']['tokens']\n",
    "lst_of_trailing_whitespaces = dataset['medical_consultations']['trailing_whitespaces']\n",
    "lst_of_labels = dataset['medical_consultations']['labels']\n",
    "print(type(lst_of_ent_tags[0][2]))\n",
    "print(type(lst_of_tokens[0][2]))\n",
    "print(type(lst_of_trailing_whitespaces[0][2]))\n",
    "print(type(lst_of_labels[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49c7e9ee-aeba-427f-8fd1-48a56759bd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', ': ', 'Hi', ', ', 'I ', 'am ', 'Christopher', 'Murillo', ', ', '\\n', 'I ', 'am ', 'experiencing ', 'a ', 'sharp ', 'pain ', 'in ', 'the ', 'lower ', 'right ', 'abdomen', ', ', 'occasionally ', 'radiating ', 'to ', 'my ', 'back', '. ', 'This ', 'symptom ', 'has ', 'been ', 'persistent ', 'for ', 'the ', 'past ', '4 ', 'days ', 'now', '. ', 'The ', 'pain ', 'is ', 'worsening ', 'over ', 'time', ', ', 'especially ', 'after ', 'eating ', 'or ', 'engaging ', 'in ', 'physical ', 'activities', '. ', 'I ', 'have ', 'noticed ', 'minor ', 'nausea', ', ', 'but ', 'no ', 'vomiting ', 'or ', 'bleeding', '. ', 'The ', 'pain ', 'is ', 'non', '-', 'specific', ', ', 'meaning ', 'it ', 'does', \"n't \", 'seem ', 'to ', 'be ', 'triggered ', 'by ', 'a ', 'specific ', 'food ', 'intake ', 'or ', 'any ', 'other ', 'activity', '. ', '\\n\\n', 'You ', 'can ', 'reach ', 'me ', 'at ', 'alvarezkenneth@gmail.com', 'or ', '493-290-9635', 'for ', 'any ', 'clarification ', 'or ', 'follow', '-', 'up ', 'questions', '.', '\\n\\n', 'I ', 'have ', 'a ', 'history ', 'of ', 'appendicitis ', 'which ', 'was ', 'surgically ', 'removed ', 'when ', 'I ', 'was ', '17', '. ', 'I ', 'have ', 'also ', 'had ', 'several ', 'episodes ', 'of ', 'stomach ', 'ulcers ', 'in ', 'the ', 'past', ', ', 'for ', 'which ', 'I ', 'have ', 'taken ', 'antacids ', 'and ', 'antibiotics ', 'as ', 'prescribed ', 'by ', 'my ', 'previous ', 'doctor', ', ', 'Dr. ', 'Emily ', 'Patel', ', ', 'whose ', 'contact ', 'information ', 'is ', 'available ', 'at ', 'https://facebook.com/healthcareprofessionals', '. ', '\\n\\n', 'Currently', ', ', 'I ', 'am ', 'not ', 'taking ', 'any ', 'medications', ', ', 'but ', 'I ', 'have ', 'an ', 'upcoming ', 'doctor', \"'s \", 'appointment ', 'with ', 'Dr. ', 'Sophia ', 'Lee ', 'on ', '6923421356', '. ', '\\n\\n', 'Can ', 'you ', 'please ', 'analyze ', 'my ', 'symptoms ', 'and ', 'help ', 'me ', 'understand ', 'what ', 'could ', 'be ', 'the ', 'underlying ', 'cause ', 'of ', 'this ', 'pain', '? ', 'What ', 'diagnostic ', 'tests ', 'would ', 'you ', 'recommend ', 'to ', 'confirm ', 'the ', 'diagnosis', '? ', 'Are ', 'there ', 'any ', 'lifestyle ', 'changes ', 'I ', 'can ', 'make ', 'to ', 'alleviate ', 'this ', 'discomfort', '? ', 'And ', 'finally', ', ', 'what ', 'are ', 'the ', 'treatment ', 'options ', 'available ', 'for ', 'this ', 'condition', '? ', 'I ', 'would ', 'appreciate ', 'it ', 'if ', 'you ', 'could ', 'send ', 'the ', 'response ', 'to ', 'my ', 'contact ', 'number ', '493-290-9635', 'for ', 'prompt ', 'action', '. ', '\\n\\n', 'I ', 'am ', 'willing ', 'to ', 'provide ', 'more ', 'information ', 'if ', 'required', '. ', 'You ', 'can ', 'also ', 'reach ', 'out ', 'to ', 'Dr. ', 'Michael ', 'Brown ', 'at ', 'michael.brown@sthealth.org ', 'or ', 'call ', 'him ', 'at ', '567', '-', '345', '-', '6789 ', 'for ', 'any ', 'additional ', 'information', '. ', 'My ', 'username ', 'is ', 'heathermartinez', ', ', 'and ', 'you ', 'can ', 'reach ', 'me ', 'anytime ', 'at ', '8223', 'Victoria', 'Row', 'or ', 'mail ', 'to ', 'St. ', 'Michael', \"'s \", 'Hospital', ', ', '1234 ', 'General ', 'Hospital ', 'Drive', ', ', 'Anytown', ', ', 'LA ', '70816', '. ', 'Thank ', 'you ', 'for ', 'your ', 'time ', 'and ', 'assistance', '.']\n",
      "6\n",
      "[False, True, False, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, False, False, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, False, False, False, False, False, False, True, True, True, True, True, False, False, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, False, True, False, False, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, True, True, True, False, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, False, False, True, True, True]\n",
      "[14, 14, 14, 14, 14, 14, 2, 9, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 14, 3, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 3, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 6, 14, 14, 14, 14, 14, 14, 14, 14, 4, 11, 11, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n"
     ]
    }
   ],
   "source": [
    "print(lst_of_tokens[0])\n",
    "print(lst_of_tokens[0].index(\"Christopher\"))\n",
    "print(lst_of_trailing_whitespaces[0])\n",
    "print(lst_of_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "387c4f7f-91e7-4db0-96d4-871c14bd8164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def reconstruct_text_from_tokens(tokens: List[str], has_trailing_space: List[bool], labels: List[int]) -> str:\n",
    "    \"\"\"\n",
    "    Reconstructs a text string from a list of tokens and a list of trailing whitespace indicators.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): List of token strings.\n",
    "        has_trailing_space (List[bool]): List of booleans where True means a space should follow the token.\n",
    "\n",
    "    Returns:\n",
    "        str: The reconstructed string with appropriate single-space separations.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the lengths of tokens and has_trailing_space do not match.\n",
    "    \"\"\"\n",
    "    if len(tokens) != len(has_trailing_space):\n",
    "        raise ValueError(\"Length of tokens and has_trailing_space must be equal.\")\n",
    "    \n",
    "    pieces = []\n",
    "    for token, has_space, label in zip(tokens, has_trailing_space, labels):\n",
    "        pieces.append(token)\n",
    "        if (label != 14):\n",
    "            pieces.append(\" \")\n",
    "\n",
    "    return \"\".join(pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b405101-738d-46cf-b533-d012a885f492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hi, I am Christopher Murillo , \n",
      "I am experiencing a sharp pain in the lower right abdomen, occasionally radiating to my back. This symptom has been persistent for the past 4 days now. The pain is worsening over time, especially after eating or engaging in physical activities. I have noticed minor nausea, but no vomiting or bleeding. The pain is non-specific, meaning it doesn't seem to be triggered by a specific food intake or any other activity. \n",
      "\n",
      "You can reach me at alvarezkenneth@gmail.com or 493-290-9635 for any clarification or follow-up questions.\n",
      "\n",
      "I have a history of appendicitis which was surgically removed when I was 17. I have also had several episodes of stomach ulcers in the past, for which I have taken antacids and antibiotics as prescribed by my previous doctor, Dr. Emily Patel, whose contact information is available at https://facebook.com/healthcareprofessionals. \n",
      "\n",
      "Currently, I am not taking any medications, but I have an upcoming doctor's appointment with Dr. Sophia Lee on 6923421356. \n",
      "\n",
      "Can you please analyze my symptoms and help me understand what could be the underlying cause of this pain? What diagnostic tests would you recommend to confirm the diagnosis? Are there any lifestyle changes I can make to alleviate this discomfort? And finally, what are the treatment options available for this condition? I would appreciate it if you could send the response to my contact number 493-290-9635 for prompt action. \n",
      "\n",
      "I am willing to provide more information if required. You can also reach out to Dr. Michael Brown at michael.brown@sthealth.org or call him at 567-345-6789 for any additional information. My username is heathermartinez , and you can reach me anytime at 8223 Victoria Row or mail to St. Michael's Hospital, 1234 General Hospital Drive, Anytown, LA 70816. Thank you for your time and assistance.\n"
     ]
    }
   ],
   "source": [
    "t = reconstruct_text_from_tokens(lst_of_tokens[0],lst_of_trailing_whitespaces[0],lst_of_labels[0])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b11673b8-20a9-4f29-9020-2327e07da5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ! I 'm Jacqueline Adams , reaching out for an online consultation due to a recurring issue with tooth sensitivity . It 's been going on for the past 6 months , and it 's been getting gradually worse . Sometimes it 's just a mild tingling sensation , but other times it 's a sharp pain . I 've noticed it 's triggered by eating or drinking something hot or cold . I 've had a few dental fillings in the past , but no major dental work . You can reach me back at christopher17@gmail.com if there 's any additional information required . I 've tried using toothpaste for sensitive teeth , but it does n't seem to be making a difference . My dentist prescribed me a fluoride rinse , but I have n't seen much improvement . In my free time , I usually play online games under the username millerchristina . I 'd appreciate it if you could recommend any products or services that are available at 6771 Johns Shores or can be shipped to my location . Could you please advise on what might be causing this sensitivity and what treatment options are available ? Would I need to come in for an in - person appointment or can we proceed with the consultation online ? Also , are there any over - the - counter medications or products you would recommend in the meantime to help alleviate the sensitivity ? Please share some useful resources and tips on my blog at https://hernandez-perez.org/listlogin.php . Additionally , you can reach me at 671.616.3448x736 if you need to clarify anything . Thank you for your time , and I look forward to your response . Dr. Smith 's dental office has an online consultation service available at dentalcareonline.com , you can also contact them at info@dentalcareonline.com or by calling 555 - 123 - 4567 . Their office is located at 123 Main St , Anytown , CA 12345 . For any urgent matters , please contact Dr. Smith 's assistant , Emily Chen at emily.chen@dentalcareonline.com or 555 - 234 - 5678 .\n"
     ]
    }
   ],
   "source": [
    "lst_of_docs = [\" \".join(lst)\n",
    "    for lst in lst_of_tokens\n",
    "]\n",
    "lst_of_docs = [\" \".join(doc.split())\n",
    "              for doc in lst_of_docs]\n",
    "print(lst_of_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e95021f-000a-46e6-bcd1-344b32fb2ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', ': ', 'Hi', ', ', 'I ', 'am ', 'Christopher', 'Murillo', ', ', '\\n', 'I ', 'am ', 'experiencing ', 'a ', 'sharp ', 'pain ', 'in ', 'the ', 'lower ', 'right ', 'abdomen', ', ', 'occasionally ', 'radiating ', 'to ', 'my ', 'back', '. ', 'This ', 'symptom ', 'has ', 'been ', 'persistent ', 'for ', 'the ', 'past ', '4 ', 'days ', 'now', '. ', 'The ', 'pain ', 'is ', 'worsening ', 'over ', 'time', ', ', 'especially ', 'after ', 'eating ', 'or ', 'engaging ', 'in ', 'physical ', 'activities', '. ', 'I ', 'have ', 'noticed ', 'minor ', 'nausea', ', ', 'but ', 'no ', 'vomiting ', 'or ', 'bleeding', '. ', 'The ', 'pain ', 'is ', 'non', '-', 'specific', ', ', 'meaning ', 'it ', 'does', \"n't \", 'seem ', 'to ', 'be ', 'triggered ', 'by ', 'a ', 'specific ', 'food ', 'intake ', 'or ', 'any ', 'other ', 'activity', '. ', '\\n\\n', 'You ', 'can ', 'reach ', 'me ', 'at ', 'alvarezkenneth@gmail.com', 'or ', '493-290-9635', 'for ', 'any ', 'clarification ', 'or ', 'follow', '-', 'up ', 'questions', '.', '\\n\\n', 'I ', 'have ', 'a ', 'history ', 'of ', 'appendicitis ', 'which ', 'was ', 'surgically ', 'removed ', 'when ', 'I ', 'was ', '17', '. ', 'I ', 'have ', 'also ', 'had ', 'several ', 'episodes ', 'of ', 'stomach ', 'ulcers ', 'in ', 'the ', 'past', ', ', 'for ', 'which ', 'I ', 'have ', 'taken ', 'antacids ', 'and ', 'antibiotics ', 'as ', 'prescribed ', 'by ', 'my ', 'previous ', 'doctor', ', ', 'Dr. ', 'Emily ', 'Patel', ', ', 'whose ', 'contact ', 'information ', 'is ', 'available ', 'at ', 'https://facebook.com/healthcareprofessionals', '. ', '\\n\\n', 'Currently', ', ', 'I ', 'am ', 'not ', 'taking ', 'any ', 'medications', ', ', 'but ', 'I ', 'have ', 'an ', 'upcoming ', 'doctor', \"'s \", 'appointment ', 'with ', 'Dr. ', 'Sophia ', 'Lee ', 'on ', '6923421356', '. ', '\\n\\n', 'Can ', 'you ', 'please ', 'analyze ', 'my ', 'symptoms ', 'and ', 'help ', 'me ', 'understand ', 'what ', 'could ', 'be ', 'the ', 'underlying ', 'cause ', 'of ', 'this ', 'pain', '? ', 'What ', 'diagnostic ', 'tests ', 'would ', 'you ', 'recommend ', 'to ', 'confirm ', 'the ', 'diagnosis', '? ', 'Are ', 'there ', 'any ', 'lifestyle ', 'changes ', 'I ', 'can ', 'make ', 'to ', 'alleviate ', 'this ', 'discomfort', '? ', 'And ', 'finally', ', ', 'what ', 'are ', 'the ', 'treatment ', 'options ', 'available ', 'for ', 'this ', 'condition', '? ', 'I ', 'would ', 'appreciate ', 'it ', 'if ', 'you ', 'could ', 'send ', 'the ', 'response ', 'to ', 'my ', 'contact ', 'number ', '493-290-9635', 'for ', 'prompt ', 'action', '. ', '\\n\\n', 'I ', 'am ', 'willing ', 'to ', 'provide ', 'more ', 'information ', 'if ', 'required', '. ', 'You ', 'can ', 'also ', 'reach ', 'out ', 'to ', 'Dr. ', 'Michael ', 'Brown ', 'at ', 'michael.brown@sthealth.org ', 'or ', 'call ', 'him ', 'at ', '567', '-', '345', '-', '6789 ', 'for ', 'any ', 'additional ', 'information', '. ', 'My ', 'username ', 'is ', 'heathermartinez', ', ', 'and ', 'you ', 'can ', 'reach ', 'me ', 'anytime ', 'at ', '8223', 'Victoria', 'Row', 'or ', 'mail ', 'to ', 'St. ', 'Michael', \"'s \", 'Hospital', ', ', '1234 ', 'General ', 'Hospital ', 'Drive', ', ', 'Anytown', ', ', 'LA ', '70816', '. ', 'Thank ', 'you ', 'for ', 'your ', 'time ', 'and ', 'assistance', '.']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['medical_consultations']['tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1abb96aa-a227-441c-8b06-4720be48390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset['medical_consultations'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5612a264-be7e-4b56-b828-3fa9b7f3951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['tokens'].apply(lambda x: \"\".join(x))\n",
    "dataset_txt=Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f67c238e-d9a2-46fa-837b-e15c203ffc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"obi/deid_roberta_i2b2\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"obi/deid_roberta_i2b2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "059b0cfa-19e7-46a3-8a5a-591338b69512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f14b4db1a0481ab156f9b92e7475d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = pipe.tokenizer  # get tokenizer from your existing pipeline\n",
    "\n",
    "results = []\n",
    "for inp in tqdm(lst_of_docs):\n",
    "    assert isinstance(inp, str), \"not a str\"\n",
    "    \n",
    "    # Truncate the input at token level\n",
    "    inputs = tokenizer(\n",
    "        inp,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    # Decode back to string after truncation\n",
    "    truncated_text = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "    # Pass the truncated string to the pipeline\n",
    "    out = pipe(truncated_text)\n",
    "    results.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45a4159c-844d-4ce4-b8ae-0e76663591d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PATIENT', 'score': np.float32(0.99691075), 'index': 6, 'word': 'Jacqueline', 'start': 13, 'end': 23}, {'entity': 'L-PATIENT', 'score': np.float32(0.9775359), 'index': 7, 'word': 'Adams', 'start': 24, 'end': 29}, {'entity': 'U-STAFF', 'score': np.float32(0.7998271), 'index': 107, 'word': 'ch', 'start': 465, 'end': 467}, {'entity': 'U-PATIENT', 'score': np.float32(0.4773383), 'index': 187, 'word': 'mill', 'start': 790, 'end': 794}, {'entity': 'B-LOC', 'score': np.float32(0.9802167), 'index': 210, 'word': '67', 'start': 897, 'end': 899}, {'entity': 'B-LOC', 'score': np.float32(0.6470808), 'index': 211, 'word': '##7', 'start': 899, 'end': 900}, {'entity': 'I-LOC', 'score': np.float32(0.5506426), 'index': 212, 'word': '##1', 'start': 900, 'end': 901}, {'entity': 'I-LOC', 'score': np.float32(0.986521), 'index': 213, 'word': 'Johns', 'start': 902, 'end': 907}, {'entity': 'L-LOC', 'score': np.float32(0.7195891), 'index': 214, 'word': 'Shore', 'start': 908, 'end': 913}, {'entity': 'U-STAFF', 'score': np.float32(0.99487984), 'index': 303, 'word': 'her', 'start': 1366, 'end': 1369}, {'entity': 'U-STAFF', 'score': np.float32(0.97739923), 'index': 304, 'word': '##nan', 'start': 1369, 'end': 1372}, {'entity': 'U-STAFF', 'score': np.float32(0.9237844), 'index': 307, 'word': 'per', 'start': 1378, 'end': 1381}, {'entity': 'U-STAFF', 'score': np.float32(0.9790348), 'index': 312, 'word': 'list', 'start': 1391, 'end': 1395}, {'entity': 'B-LOC', 'score': np.float32(0.7664057), 'index': 326, 'word': '67', 'start': 1441, 'end': 1443}, {'entity': 'I-PHONE', 'score': np.float32(0.5780116), 'index': 328, 'word': '.', 'start': 1444, 'end': 1445}, {'entity': 'I-PHONE', 'score': np.float32(0.5422127), 'index': 329, 'word': '61', 'start': 1446, 'end': 1448}, {'entity': 'I-PHONE', 'score': np.float32(0.59821856), 'index': 330, 'word': '##6', 'start': 1448, 'end': 1449}, {'entity': 'I-PHONE', 'score': np.float32(0.72108173), 'index': 331, 'word': '.', 'start': 1449, 'end': 1450}, {'entity': 'L-PHONE', 'score': np.float32(0.89027214), 'index': 332, 'word': '34', 'start': 1451, 'end': 1453}, {'entity': 'L-PHONE', 'score': np.float32(0.7539926), 'index': 333, 'word': '##48', 'start': 1453, 'end': 1455}, {'entity': 'L-PHONE', 'score': np.float32(0.3941702), 'index': 334, 'word': '##x', 'start': 1455, 'end': 1456}, {'entity': 'L-PHONE', 'score': np.float32(0.28808916), 'index': 335, 'word': '##7', 'start': 1456, 'end': 1457}, {'entity': 'U-STAFF', 'score': np.float32(0.97552663), 'index': 362, 'word': 'Smith', 'start': 1559, 'end': 1564}, {'entity': 'U-PHONE', 'score': np.float32(0.99310076), 'index': 398, 'word': '55', 'start': 1726, 'end': 1728}, {'entity': 'U-PHONE', 'score': np.float32(0.6772597), 'index': 399, 'word': '##5', 'start': 1728, 'end': 1729}, {'entity': 'L-PHONE', 'score': np.float32(0.44468188), 'index': 401, 'word': '123', 'start': 1732, 'end': 1735}, {'entity': 'L-PHONE', 'score': np.float32(0.71347773), 'index': 403, 'word': '45', 'start': 1738, 'end': 1740}, {'entity': 'B-LOC', 'score': np.float32(0.9976165), 'index': 412, 'word': '123', 'start': 1771, 'end': 1774}, {'entity': 'I-LOC', 'score': np.float32(0.996711), 'index': 413, 'word': 'Main', 'start': 1775, 'end': 1779}, {'entity': 'L-LOC', 'score': np.float32(0.99892586), 'index': 414, 'word': 'St', 'start': 1780, 'end': 1782}, {'entity': 'U-LOC', 'score': np.float32(0.99708456), 'index': 416, 'word': 'Any', 'start': 1784, 'end': 1787}, {'entity': 'L-LOC', 'score': np.float32(0.98865324), 'index': 417, 'word': '##town', 'start': 1787, 'end': 1791}, {'entity': 'U-LOC', 'score': np.float32(0.99938965), 'index': 419, 'word': 'CA', 'start': 1793, 'end': 1795}, {'entity': 'U-LOC', 'score': np.float32(0.9993125), 'index': 420, 'word': '123', 'start': 1796, 'end': 1799}, {'entity': 'U-STAFF', 'score': np.float32(0.9983557), 'index': 432, 'word': 'Smith', 'start': 1846, 'end': 1851}, {'entity': 'B-STAFF', 'score': np.float32(0.7846137), 'index': 437, 'word': 'Emily', 'start': 1867, 'end': 1872}, {'entity': 'L-PATIENT', 'score': np.float32(0.7293447), 'index': 438, 'word': 'Chen', 'start': 1873, 'end': 1877}, {'entity': 'U-STAFF', 'score': np.float32(0.9827481), 'index': 443, 'word': 'ch', 'start': 1888, 'end': 1890}, {'entity': 'B-PATORG', 'score': np.float32(0.26081634), 'index': 446, 'word': 'dental', 'start': 1895, 'end': 1901}, {'entity': 'U-PHONE', 'score': np.float32(0.98377985), 'index': 453, 'word': '55', 'start': 1920, 'end': 1922}, {'entity': 'U-PHONE', 'score': np.float32(0.7040848), 'index': 454, 'word': '##5', 'start': 1922, 'end': 1923}, {'entity': 'I-PHONE', 'score': np.float32(0.29677773), 'index': 455, 'word': '-', 'start': 1924, 'end': 1925}, {'entity': 'L-PHONE', 'score': np.float32(0.81762034), 'index': 456, 'word': '234', 'start': 1926, 'end': 1929}, {'entity': 'I-PHONE', 'score': np.float32(0.40407184), 'index': 457, 'word': '-', 'start': 1930, 'end': 1931}, {'entity': 'L-PHONE', 'score': np.float32(0.9215562), 'index': 458, 'word': '56', 'start': 1932, 'end': 1934}]\n"
     ]
    }
   ],
   "source": [
    "print(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dafdeea7-40a4-4b5e-8cba-380a63d91206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_entities(tokens, ner_tags):\n",
    "    \"\"\"\n",
    "    this function does something\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): labeled tokens\n",
    "        ner_tags (List[str]) Maybe ground truth labels? but idk\n",
    "\n",
    "    Returns:\n",
    "        entities (List[dict[str, str])): ????\n",
    "    \"\"\"\n",
    "    label_map = dataset['medical_consultations'].features['ent_tags']\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    for token, tag in zip(tokens, ner_tags):\n",
    "        tag_label = label_map[tag]\n",
    "        if tag_label.startswith(\"B-\"):\n",
    "            if current_entity:\n",
    "                entities.append(\" \".join(current_entity))\n",
    "            current_entity = [token]\n",
    "        elif tag_label.startswith(\"I-\") and current_entity:\n",
    "            current_entity.append(token)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entities.append(\" \".join(current_entity))\n",
    "                current_entity = []\n",
    "    if current_entity:\n",
    "        entities.append(\" \".join(current_entity))\n",
    "    return entities\n",
    "\n",
    "# results3 = pipe(truncated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4fc03962-a843-4727-84c6-dbd39db23775",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Sequence' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m get_true_entities(lst_of_ent_tags, results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mget_true_entities\u001b[39m\u001b[34m(tokens, ner_tags)\u001b[39m\n\u001b[32m     14\u001b[39m current_entity = []\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token, tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tokens, ner_tags):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     tag_label = label_map[tag]\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tag_label.startswith(\u001b[33m\"\u001b[39m\u001b[33mB-\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     18\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m current_entity:\n",
      "\u001b[31mTypeError\u001b[39m: 'Sequence' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "get_true_entities(lst_of_ent_tags, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84b5bcfd-9904-40be-b076-f75a0d3d8c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a3422f0e3d44b98807f5a00251bf58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m         fp += \u001b[32m1\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# print(tag)\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# lst_of_lst_of_dict = \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     lst_of_lst_of_dict[i].remove(tag)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m lst_of_lst_of_dict[i]: \n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tag == \u001b[33m'\u001b[39m\u001b[33mO\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "lst_of_lst_of_dict = copy.deepcopy(lst_of_ent_tags)\n",
    "tn,tp = 0,0\n",
    "fn,fp = 0,0\n",
    "for i, lst in tqdm(enumerate(results)):\n",
    "    for dictionary in lst:\n",
    "        start_idx = dictionary['start']\n",
    "        end_idx = dictionary ['end']\n",
    "        doc_i = lst_of_docs[i]\n",
    "        tok = doc_i[start_idx:end_idx+1]\n",
    "        ent_tags = lst_of_ent_tags[i]\n",
    "        entity = dictionary['start']\n",
    "        doc_tok = lst_of_tokens[i]\n",
    "        try:\n",
    "            \n",
    "            tok_idx = doc_tok.index(tok.strip())\n",
    "        except ValueError as ve:\n",
    "            fp += 1\n",
    "            continue\n",
    "# print(start_idx)\n",
    "# print(end_idx)\n",
    "# print(doc_i)\n",
    "# print(tok)\n",
    "# print(ent_tags)\n",
    "# print(entity)\n",
    "# print(doc_tok)\n",
    "# raise ValueError(f'{start_idx} oh no') from ve\n",
    "\n",
    "        try:\n",
    "            \n",
    "            tag = ent_tags[tok_idx]\n",
    "        except ValueError as ve:\n",
    "            print(doc_tok)\n",
    "            print(ent_tags)\n",
    "            assert len(doc_tok) == len(ent_tags), f'{len(doc_tok)} != {len(ent_tags)}'\n",
    "        is_pii = tag != 'O'\n",
    "        if is_pii: \n",
    "            tp += 1 \n",
    "        else:\n",
    "            \n",
    "            fp += 1\n",
    "        # print(tag)\n",
    "        # lst_of_lst_of_dict = \n",
    "        lst_of_lst_of_dict[i].remove(tag)\n",
    "    for tag in lst_of_lst_of_dict[i]: \n",
    "        if tag == 'O':\n",
    "            \n",
    "            tn += 1\n",
    "        else:\n",
    "             \n",
    "             fn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86f959ce-8ce7-4c24-8343-16f28d839575",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lst_of_tokenized_pii' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m i = \u001b[32m0\u001b[39m\n\u001b[32m      2\u001b[39m tp, fp = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lst_of_pii, pii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, lst_of_tokenized_pii):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m dictionary \u001b[38;5;129;01min\u001b[39;00m lst_of_pii:\n\u001b[32m      5\u001b[39m         word = dictionary[\u001b[33m'\u001b[39m\u001b[33mword\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'lst_of_tokenized_pii' is not defined"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "tp, fp = 0, 0\n",
    "for lst_of_pii, pii in zip(results, lst_of_tokenized_pii):\n",
    "    for dictionary in lst_of_pii:\n",
    "        word = dictionary['word']\n",
    "        if word in pii:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1 \n",
    "        lst_of_tokenized_docs[i].remove(word)\n",
    "    i += 1\n",
    "print(f\"TP: {tp}\\nFP: {fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6c1382e-656e-410c-acb4-4bf5ef24fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_of_docs = []\n",
    "for tok_lst, space_lst, label_lst in zip(lst_of_tokens, lst_of_trailing_whitespaces, lst_of_labels):\n",
    "    doc = reconstruct_text_from_tokens(tok_lst, space_lst, label_lst)\n",
    "    lst_of_docs.append(doc)\n",
    "lst_of_tokenized_docs = []\n",
    "for doc in lst_of_docs:\n",
    "    tokenized_doc = tokenizer.tokenize(doc)\n",
    "    if isinstance(tokenized_doc, list):\n",
    "        lst_of_tokenized_docs.append(tokenized_doc)\n",
    "    else:\n",
    "        lst_of_tokenized_docs.append([tokenized_doc])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eacdfc47-fb3d-4328-a444-8fc930875c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_of_pii = []\n",
    "for tok_lst, lab_lst in zip(lst_of_tokens, lst_of_labels):\n",
    "    pii = []\n",
    "    for tok, lab in zip(tok_lst, lab_lst):\n",
    "        if lab != 14:\n",
    "            pii.append(tok)\n",
    "    lst_of_pii.append(pii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73a5eaca-8d02-4b76-b338-36566bf3eff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Christopher',\n",
       " 'Mu',\n",
       " '##rill',\n",
       " '##o',\n",
       " 'al',\n",
       " '##var',\n",
       " '##ez',\n",
       " '##ken',\n",
       " '##net',\n",
       " '##h',\n",
       " '@',\n",
       " 'g',\n",
       " '##mail',\n",
       " '.',\n",
       " 'com',\n",
       " '49',\n",
       " '##3',\n",
       " '-',\n",
       " '290',\n",
       " '-',\n",
       " '96',\n",
       " '##35',\n",
       " '49',\n",
       " '##3',\n",
       " '-',\n",
       " '290',\n",
       " '-',\n",
       " '96',\n",
       " '##35',\n",
       " 'heat',\n",
       " '##her',\n",
       " '##mart',\n",
       " '##ine',\n",
       " '##z',\n",
       " '82',\n",
       " '##23',\n",
       " 'Victoria',\n",
       " 'Row']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_of_tokenized_pii = []\n",
    "for lst in lst_of_pii:\n",
    "    tok_pii = []\n",
    "    for pii in lst: \n",
    "        tok = tokenizer.tokenize(pii)\n",
    "        if isinstance(tok, list):\n",
    "            for t in tok: \n",
    "                tok_pii.append(t)\n",
    "        else:\n",
    "            tok_pii.append(tok)\n",
    "    lst_of_tokenized_pii.append(tok_pii)\n",
    "print(len(lst_of_tokenized_pii))\n",
    "lst_of_tokenized_pii[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fbd5fe37-38d7-41b5-a9d4-894325b69ffa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m     fn += \u001b[38;5;28mlen\u001b[39m(lst)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m lst:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m         lst_of_tokenized_docs[i].remove(tok)\n\u001b[32m      7\u001b[39m     i += \u001b[32m1\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lst \u001b[38;5;129;01min\u001b[39;00m lst_of_tokenized_docs:\n",
      "\u001b[31mValueError\u001b[39m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "fn,tn = 0,0\n",
    "for lst in copy.deepcopy(lst_of_tokenized_pii): \n",
    "    fn += len(lst)\n",
    "    for tok in lst:\n",
    "        lst_of_tokenized_docs[i].remove(tok)\n",
    "    i += 1 \n",
    "for lst in lst_of_tokenized_docs:\n",
    "    tn += len(lst)\n",
    "\n",
    "print(f\"FN: {fn}\\nTN: {tn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22b620b5-b43f-4c43-83ae-657507dc24af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 0\n",
      "TN: 2490017\n",
      "FP: 0\n",
      "FN: 278844\n"
     ]
    }
   ],
   "source": [
    "print(f\"TP: {tp}\\nTN: {tn}\\nFP: {fp}\\nFN: {fn}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb28124b-ab9a-4133-8f9d-eefe2a5e3b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Christopher',\n",
       " 'Mu',\n",
       " '##rill',\n",
       " '##o',\n",
       " 'al',\n",
       " '##var',\n",
       " '##ez',\n",
       " '##ken',\n",
       " '##net',\n",
       " '##h',\n",
       " '@',\n",
       " 'g',\n",
       " '##mail',\n",
       " '.',\n",
       " 'com',\n",
       " '49',\n",
       " '##3',\n",
       " '-',\n",
       " '290',\n",
       " '-',\n",
       " '96',\n",
       " '##35',\n",
       " '49',\n",
       " '##3',\n",
       " '-',\n",
       " '290',\n",
       " '-',\n",
       " '96',\n",
       " '##35',\n",
       " 'heat',\n",
       " '##her',\n",
       " '##mart',\n",
       " '##ine',\n",
       " '##z',\n",
       " '82',\n",
       " '##23',\n",
       " 'Victoria',\n",
       " 'Row']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_of_tokenized_pii = []\n",
    "for lst in lst_of_pii:\n",
    "    tok_pii = []\n",
    "    for pii in lst: \n",
    "        tok = tokenizer.tokenize(pii)\n",
    "        if isinstance(tok, list):\n",
    "            for t in tok: \n",
    "                tok_pii.append(t)\n",
    "        else:\n",
    "            tok_pii.append(tok)\n",
    "    lst_of_tokenized_pii.append(tok_pii)\n",
    "print(len(lst_of_tokenized_pii))\n",
    "lst_of_tokenized_pii[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81494c-fe3c-4933-ae6b-428e7e8fb9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b448aa77-dc4d-45e1-a2b9-a080588feedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca979d-ffd5-4a22-bca7-e29286d40d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stackleberggametheory",
   "language": "python",
   "name": "stackleberggametheory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
