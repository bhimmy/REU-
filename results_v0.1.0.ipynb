{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b3df49-3a66-4333-a425-f6b533d49120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import torch\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "from datasets import load_dataset, Dataset\n",
    "from dotenv import load_dotenv\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "pipe = pipeline(\"token-classification\", model=\"obi/deid_bert_i2b2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66c6a98f-4885-49df-b6c8-7bcbc9689c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mks-logic/SPY\", trust_remote_code=True, faker_random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd0d5e2-a112-4d1c-9e6d-6b91c1fe78db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'legal_questions': ['tokens', 'trailing_whitespaces', 'labels', 'ent_tags'], 'medical_consultations': ['tokens', 'trailing_whitespaces', 'labels', 'ent_tags']}\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'bool'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "for x in dataset['medical_consultations']:\n",
    "    # print(x)\n",
    "    break\n",
    "print(dataset.column_names)\n",
    "lst_of_ent_tags = dataset['medical_consultations']['ent_tags']\n",
    "lst_of_tokens = dataset['medical_consultations']['tokens']\n",
    "lst_of_trailing_whitespaces = dataset['medical_consultations']['trailing_whitespaces']\n",
    "lst_of_labels = dataset['medical_consultations']['labels']\n",
    "print(type(lst_of_ent_tags[0][2]))\n",
    "print(type(lst_of_tokens[0][2]))\n",
    "print(type(lst_of_trailing_whitespaces[0][2]))\n",
    "print(type(lst_of_labels[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c7e9ee-aeba-427f-8fd1-48a56759bd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', ': ', 'Hi', ', ', 'I ', 'am ', 'Christopher', 'Murillo', ', ', '\\n', 'I ', 'am ', 'experiencing ', 'a ', 'sharp ', 'pain ', 'in ', 'the ', 'lower ', 'right ', 'abdomen', ', ', 'occasionally ', 'radiating ', 'to ', 'my ', 'back', '. ', 'This ', 'symptom ', 'has ', 'been ', 'persistent ', 'for ', 'the ', 'past ', '4 ', 'days ', 'now', '. ', 'The ', 'pain ', 'is ', 'worsening ', 'over ', 'time', ', ', 'especially ', 'after ', 'eating ', 'or ', 'engaging ', 'in ', 'physical ', 'activities', '. ', 'I ', 'have ', 'noticed ', 'minor ', 'nausea', ', ', 'but ', 'no ', 'vomiting ', 'or ', 'bleeding', '. ', 'The ', 'pain ', 'is ', 'non', '-', 'specific', ', ', 'meaning ', 'it ', 'does', \"n't \", 'seem ', 'to ', 'be ', 'triggered ', 'by ', 'a ', 'specific ', 'food ', 'intake ', 'or ', 'any ', 'other ', 'activity', '. ', '\\n\\n', 'You ', 'can ', 'reach ', 'me ', 'at ', 'alvarezkenneth@gmail.com', 'or ', '493-290-9635', 'for ', 'any ', 'clarification ', 'or ', 'follow', '-', 'up ', 'questions', '.', '\\n\\n', 'I ', 'have ', 'a ', 'history ', 'of ', 'appendicitis ', 'which ', 'was ', 'surgically ', 'removed ', 'when ', 'I ', 'was ', '17', '. ', 'I ', 'have ', 'also ', 'had ', 'several ', 'episodes ', 'of ', 'stomach ', 'ulcers ', 'in ', 'the ', 'past', ', ', 'for ', 'which ', 'I ', 'have ', 'taken ', 'antacids ', 'and ', 'antibiotics ', 'as ', 'prescribed ', 'by ', 'my ', 'previous ', 'doctor', ', ', 'Dr. ', 'Emily ', 'Patel', ', ', 'whose ', 'contact ', 'information ', 'is ', 'available ', 'at ', 'https://facebook.com/healthcareprofessionals', '. ', '\\n\\n', 'Currently', ', ', 'I ', 'am ', 'not ', 'taking ', 'any ', 'medications', ', ', 'but ', 'I ', 'have ', 'an ', 'upcoming ', 'doctor', \"'s \", 'appointment ', 'with ', 'Dr. ', 'Sophia ', 'Lee ', 'on ', '6923421356', '. ', '\\n\\n', 'Can ', 'you ', 'please ', 'analyze ', 'my ', 'symptoms ', 'and ', 'help ', 'me ', 'understand ', 'what ', 'could ', 'be ', 'the ', 'underlying ', 'cause ', 'of ', 'this ', 'pain', '? ', 'What ', 'diagnostic ', 'tests ', 'would ', 'you ', 'recommend ', 'to ', 'confirm ', 'the ', 'diagnosis', '? ', 'Are ', 'there ', 'any ', 'lifestyle ', 'changes ', 'I ', 'can ', 'make ', 'to ', 'alleviate ', 'this ', 'discomfort', '? ', 'And ', 'finally', ', ', 'what ', 'are ', 'the ', 'treatment ', 'options ', 'available ', 'for ', 'this ', 'condition', '? ', 'I ', 'would ', 'appreciate ', 'it ', 'if ', 'you ', 'could ', 'send ', 'the ', 'response ', 'to ', 'my ', 'contact ', 'number ', '493-290-9635', 'for ', 'prompt ', 'action', '. ', '\\n\\n', 'I ', 'am ', 'willing ', 'to ', 'provide ', 'more ', 'information ', 'if ', 'required', '. ', 'You ', 'can ', 'also ', 'reach ', 'out ', 'to ', 'Dr. ', 'Michael ', 'Brown ', 'at ', 'michael.brown@sthealth.org ', 'or ', 'call ', 'him ', 'at ', '567', '-', '345', '-', '6789 ', 'for ', 'any ', 'additional ', 'information', '. ', 'My ', 'username ', 'is ', 'heathermartinez', ', ', 'and ', 'you ', 'can ', 'reach ', 'me ', 'anytime ', 'at ', '8223', 'Victoria', 'Row', 'or ', 'mail ', 'to ', 'St. ', 'Michael', \"'s \", 'Hospital', ', ', '1234 ', 'General ', 'Hospital ', 'Drive', ', ', 'Anytown', ', ', 'LA ', '70816', '. ', 'Thank ', 'you ', 'for ', 'your ', 'time ', 'and ', 'assistance', '.']\n",
      "6\n",
      "[False, True, False, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, False, False, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, False, False, False, False, False, False, True, True, True, True, True, False, False, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, False, True, False, False, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, True, True, True, False, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, False, False, True, True, True]\n",
      "[14, 14, 14, 14, 14, 14, 2, 9, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 14, 3, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 3, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 6, 14, 14, 14, 14, 14, 14, 14, 14, 4, 11, 11, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n"
     ]
    }
   ],
   "source": [
    "print(lst_of_tokens[0])\n",
    "print(lst_of_tokens[0].index(\"Christopher\"))\n",
    "print(lst_of_trailing_whitespaces[0])\n",
    "print(lst_of_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "387c4f7f-91e7-4db0-96d4-871c14bd8164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def reconstruct_text_from_tokens(tokens: List[str], has_trailing_space: List[bool], labels: List[int]) -> str:\n",
    "    \"\"\"\n",
    "    Reconstructs a text string from a list of tokens and a list of trailing whitespace indicators.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): List of token strings.\n",
    "        has_trailing_space (List[bool]): List of booleans where True means a space should follow the token.\n",
    "\n",
    "    Returns:\n",
    "        str: The reconstructed string with appropriate single-space separations.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the lengths of tokens and has_trailing_space do not match.\n",
    "    \"\"\"\n",
    "    if len(tokens) != len(has_trailing_space):\n",
    "        raise ValueError(\"Length of tokens and has_trailing_space must be equal.\")\n",
    "    \n",
    "    pieces = []\n",
    "    for token, has_space, label in zip(tokens, has_trailing_space, labels):\n",
    "        pieces.append(token)\n",
    "        if (label != 14):\n",
    "            pieces.append(\" \")\n",
    "\n",
    "    return \"\".join(pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b405101-738d-46cf-b533-d012a885f492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hi, I am Christopher Murillo , \n",
      "I am experiencing a sharp pain in the lower right abdomen, occasionally radiating to my back. This symptom has been persistent for the past 4 days now. The pain is worsening over time, especially after eating or engaging in physical activities. I have noticed minor nausea, but no vomiting or bleeding. The pain is non-specific, meaning it doesn't seem to be triggered by a specific food intake or any other activity. \n",
      "\n",
      "You can reach me at alvarezkenneth@gmail.com or 493-290-9635 for any clarification or follow-up questions.\n",
      "\n",
      "I have a history of appendicitis which was surgically removed when I was 17. I have also had several episodes of stomach ulcers in the past, for which I have taken antacids and antibiotics as prescribed by my previous doctor, Dr. Emily Patel, whose contact information is available at https://facebook.com/healthcareprofessionals. \n",
      "\n",
      "Currently, I am not taking any medications, but I have an upcoming doctor's appointment with Dr. Sophia Lee on 6923421356. \n",
      "\n",
      "Can you please analyze my symptoms and help me understand what could be the underlying cause of this pain? What diagnostic tests would you recommend to confirm the diagnosis? Are there any lifestyle changes I can make to alleviate this discomfort? And finally, what are the treatment options available for this condition? I would appreciate it if you could send the response to my contact number 493-290-9635 for prompt action. \n",
      "\n",
      "I am willing to provide more information if required. You can also reach out to Dr. Michael Brown at michael.brown@sthealth.org or call him at 567-345-6789 for any additional information. My username is heathermartinez , and you can reach me anytime at 8223 Victoria Row or mail to St. Michael's Hospital, 1234 General Hospital Drive, Anytown, LA 70816. Thank you for your time and assistance.\n"
     ]
    }
   ],
   "source": [
    "t = reconstruct_text_from_tokens(lst_of_tokens[0],lst_of_trailing_whitespaces[0],lst_of_labels[0])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f67c238e-d9a2-46fa-837b-e15c203ffc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"obi/deid_roberta_i2b2\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"obi/deid_roberta_i2b2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6c1382e-656e-410c-acb4-4bf5ef24fdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "lst_of_docs = []\n",
    "for tok_lst, space_lst, label_lst in zip(lst_of_tokens, lst_of_trailing_whitespaces, lst_of_labels):\n",
    "    doc = reconstruct_text_from_tokens(tok_lst, space_lst, label_lst)\n",
    "    lst_of_docs.append(doc)\n",
    "    \n",
    "lst_of_tokenized_docs = []\n",
    "for doc in lst_of_docs:\n",
    "    tokenized_doc = tokenizer.tokenize(doc)\n",
    "\n",
    "    if isinstance(tokenized_doc, list):\n",
    "        lst_of_tokenized_docs.append(tokenized_doc[:512])\n",
    "    else:\n",
    "        lst_of_tokenized_docs.append([tokenized_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f503ef-af4f-43a9-9d56-11c62fa9647f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b11673b8-20a9-4f29-9020-2327e07da5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst_of_docs = [\" \".join(lst)\n",
    "#     for lst in lst_of_tokens\n",
    "# ]\n",
    "# lst_of_docs = [\" \".join(doc.split())\n",
    "#               for doc in lst_of_docs]\n",
    "# print(lst_of_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e95021f-000a-46e6-bcd1-344b32fb2ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', ': ', 'Hi', ', ', 'I ', 'am ', 'Christopher', 'Murillo', ', ', '\\n', 'I ', 'am ', 'experiencing ', 'a ', 'sharp ', 'pain ', 'in ', 'the ', 'lower ', 'right ', 'abdomen', ', ', 'occasionally ', 'radiating ', 'to ', 'my ', 'back', '. ', 'This ', 'symptom ', 'has ', 'been ', 'persistent ', 'for ', 'the ', 'past ', '4 ', 'days ', 'now', '. ', 'The ', 'pain ', 'is ', 'worsening ', 'over ', 'time', ', ', 'especially ', 'after ', 'eating ', 'or ', 'engaging ', 'in ', 'physical ', 'activities', '. ', 'I ', 'have ', 'noticed ', 'minor ', 'nausea', ', ', 'but ', 'no ', 'vomiting ', 'or ', 'bleeding', '. ', 'The ', 'pain ', 'is ', 'non', '-', 'specific', ', ', 'meaning ', 'it ', 'does', \"n't \", 'seem ', 'to ', 'be ', 'triggered ', 'by ', 'a ', 'specific ', 'food ', 'intake ', 'or ', 'any ', 'other ', 'activity', '. ', '\\n\\n', 'You ', 'can ', 'reach ', 'me ', 'at ', 'alvarezkenneth@gmail.com', 'or ', '493-290-9635', 'for ', 'any ', 'clarification ', 'or ', 'follow', '-', 'up ', 'questions', '.', '\\n\\n', 'I ', 'have ', 'a ', 'history ', 'of ', 'appendicitis ', 'which ', 'was ', 'surgically ', 'removed ', 'when ', 'I ', 'was ', '17', '. ', 'I ', 'have ', 'also ', 'had ', 'several ', 'episodes ', 'of ', 'stomach ', 'ulcers ', 'in ', 'the ', 'past', ', ', 'for ', 'which ', 'I ', 'have ', 'taken ', 'antacids ', 'and ', 'antibiotics ', 'as ', 'prescribed ', 'by ', 'my ', 'previous ', 'doctor', ', ', 'Dr. ', 'Emily ', 'Patel', ', ', 'whose ', 'contact ', 'information ', 'is ', 'available ', 'at ', 'https://facebook.com/healthcareprofessionals', '. ', '\\n\\n', 'Currently', ', ', 'I ', 'am ', 'not ', 'taking ', 'any ', 'medications', ', ', 'but ', 'I ', 'have ', 'an ', 'upcoming ', 'doctor', \"'s \", 'appointment ', 'with ', 'Dr. ', 'Sophia ', 'Lee ', 'on ', '6923421356', '. ', '\\n\\n', 'Can ', 'you ', 'please ', 'analyze ', 'my ', 'symptoms ', 'and ', 'help ', 'me ', 'understand ', 'what ', 'could ', 'be ', 'the ', 'underlying ', 'cause ', 'of ', 'this ', 'pain', '? ', 'What ', 'diagnostic ', 'tests ', 'would ', 'you ', 'recommend ', 'to ', 'confirm ', 'the ', 'diagnosis', '? ', 'Are ', 'there ', 'any ', 'lifestyle ', 'changes ', 'I ', 'can ', 'make ', 'to ', 'alleviate ', 'this ', 'discomfort', '? ', 'And ', 'finally', ', ', 'what ', 'are ', 'the ', 'treatment ', 'options ', 'available ', 'for ', 'this ', 'condition', '? ', 'I ', 'would ', 'appreciate ', 'it ', 'if ', 'you ', 'could ', 'send ', 'the ', 'response ', 'to ', 'my ', 'contact ', 'number ', '493-290-9635', 'for ', 'prompt ', 'action', '. ', '\\n\\n', 'I ', 'am ', 'willing ', 'to ', 'provide ', 'more ', 'information ', 'if ', 'required', '. ', 'You ', 'can ', 'also ', 'reach ', 'out ', 'to ', 'Dr. ', 'Michael ', 'Brown ', 'at ', 'michael.brown@sthealth.org ', 'or ', 'call ', 'him ', 'at ', '567', '-', '345', '-', '6789 ', 'for ', 'any ', 'additional ', 'information', '. ', 'My ', 'username ', 'is ', 'heathermartinez', ', ', 'and ', 'you ', 'can ', 'reach ', 'me ', 'anytime ', 'at ', '8223', 'Victoria', 'Row', 'or ', 'mail ', 'to ', 'St. ', 'Michael', \"'s \", 'Hospital', ', ', '1234 ', 'General ', 'Hospital ', 'Drive', ', ', 'Anytown', ', ', 'LA ', '70816', '. ', 'Thank ', 'you ', 'for ', 'your ', 'time ', 'and ', 'assistance', '.']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['medical_consultations']['tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1abb96aa-a227-441c-8b06-4720be48390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset['medical_consultations'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5612a264-be7e-4b56-b828-3fa9b7f3951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['tokens'].apply(lambda x: \"\".join(x))\n",
    "dataset_txt=Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "059b0cfa-19e7-46a3-8a5a-591338b69512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6502c2a0c74185baf57770849edd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = pipe.tokenizer  # get tokenizer from your existing pipeline\n",
    "\n",
    "results = []\n",
    "for inp in tqdm(lst_of_docs):\n",
    "    assert isinstance(inp, str), \"not a str\"\n",
    "    \n",
    "    # Truncate the input at token level\n",
    "    inputs = tokenizer(\n",
    "        inp,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    # Decode back to string after truncation\n",
    "    truncated_text = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "    # Pass the truncated string to the pipeline\n",
    "    out = pipe(truncated_text)\n",
    "    results.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45a4159c-844d-4ce4-b8ae-0e76663591d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PATIENT', 'score': np.float32(0.99694294), 'index': 6, 'word': 'Jacqueline', 'start': 13, 'end': 23}, {'entity': 'L-PATIENT', 'score': np.float32(0.9817825), 'index': 7, 'word': 'Adams', 'start': 24, 'end': 29}, {'entity': 'U-STAFF', 'score': np.float32(0.8695738), 'index': 107, 'word': 'ch', 'start': 465, 'end': 467}, {'entity': 'U-PATIENT', 'score': np.float32(0.33063716), 'index': 185, 'word': 'mill', 'start': 788, 'end': 792}, {'entity': 'B-LOC', 'score': np.float32(0.9791631), 'index': 208, 'word': '67', 'start': 895, 'end': 897}, {'entity': 'B-LOC', 'score': np.float32(0.58527845), 'index': 209, 'word': '##7', 'start': 897, 'end': 898}, {'entity': 'I-LOC', 'score': np.float32(0.48647797), 'index': 210, 'word': '##1', 'start': 898, 'end': 899}, {'entity': 'I-LOC', 'score': np.float32(0.9896563), 'index': 211, 'word': 'Johns', 'start': 900, 'end': 905}, {'entity': 'L-LOC', 'score': np.float32(0.8167179), 'index': 212, 'word': 'Shore', 'start': 906, 'end': 911}, {'entity': 'U-STAFF', 'score': np.float32(0.99704), 'index': 301, 'word': 'her', 'start': 1364, 'end': 1367}, {'entity': 'U-STAFF', 'score': np.float32(0.97711074), 'index': 302, 'word': '##nan', 'start': 1367, 'end': 1370}, {'entity': 'U-STAFF', 'score': np.float32(0.87923807), 'index': 305, 'word': 'per', 'start': 1376, 'end': 1379}, {'entity': 'U-STAFF', 'score': np.float32(0.74805224), 'index': 310, 'word': 'list', 'start': 1389, 'end': 1393}, {'entity': 'B-LOC', 'score': np.float32(0.7466817), 'index': 324, 'word': '67', 'start': 1439, 'end': 1441}, {'entity': 'I-PHONE', 'score': np.float32(0.6172631), 'index': 326, 'word': '.', 'start': 1442, 'end': 1443}, {'entity': 'I-PHONE', 'score': np.float32(0.47052485), 'index': 327, 'word': '61', 'start': 1444, 'end': 1446}, {'entity': 'I-PHONE', 'score': np.float32(0.6033465), 'index': 328, 'word': '##6', 'start': 1446, 'end': 1447}, {'entity': 'I-PHONE', 'score': np.float32(0.67599183), 'index': 329, 'word': '.', 'start': 1447, 'end': 1448}, {'entity': 'L-PHONE', 'score': np.float32(0.9162607), 'index': 330, 'word': '34', 'start': 1449, 'end': 1451}, {'entity': 'L-PHONE', 'score': np.float32(0.67801344), 'index': 331, 'word': '##48', 'start': 1451, 'end': 1453}, {'entity': 'L-PHONE', 'score': np.float32(0.3804827), 'index': 332, 'word': '##x', 'start': 1453, 'end': 1454}, {'entity': 'L-PHONE', 'score': np.float32(0.37130845), 'index': 333, 'word': '##7', 'start': 1454, 'end': 1455}, {'entity': 'U-STAFF', 'score': np.float32(0.95115364), 'index': 360, 'word': 'Smith', 'start': 1557, 'end': 1562}, {'entity': 'U-PHONE', 'score': np.float32(0.99500823), 'index': 396, 'word': '55', 'start': 1724, 'end': 1726}, {'entity': 'U-PHONE', 'score': np.float32(0.80208564), 'index': 397, 'word': '##5', 'start': 1726, 'end': 1727}, {'entity': 'L-PHONE', 'score': np.float32(0.61767447), 'index': 399, 'word': '123', 'start': 1730, 'end': 1733}, {'entity': 'L-PHONE', 'score': np.float32(0.72793007), 'index': 401, 'word': '45', 'start': 1736, 'end': 1738}, {'entity': 'B-LOC', 'score': np.float32(0.9972269), 'index': 410, 'word': '123', 'start': 1769, 'end': 1772}, {'entity': 'I-LOC', 'score': np.float32(0.9969103), 'index': 411, 'word': 'Main', 'start': 1773, 'end': 1777}, {'entity': 'L-LOC', 'score': np.float32(0.99876547), 'index': 412, 'word': 'St', 'start': 1778, 'end': 1780}, {'entity': 'U-LOC', 'score': np.float32(0.9981091), 'index': 414, 'word': 'Any', 'start': 1782, 'end': 1785}, {'entity': 'L-LOC', 'score': np.float32(0.9589808), 'index': 415, 'word': '##town', 'start': 1785, 'end': 1789}, {'entity': 'U-LOC', 'score': np.float32(0.99937856), 'index': 417, 'word': 'CA', 'start': 1791, 'end': 1793}, {'entity': 'U-LOC', 'score': np.float32(0.9992818), 'index': 418, 'word': '123', 'start': 1794, 'end': 1797}, {'entity': 'U-STAFF', 'score': np.float32(0.99897635), 'index': 430, 'word': 'Smith', 'start': 1844, 'end': 1849}, {'entity': 'B-STAFF', 'score': np.float32(0.8005625), 'index': 435, 'word': 'Emily', 'start': 1865, 'end': 1870}, {'entity': 'L-STAFF', 'score': np.float32(0.5319459), 'index': 436, 'word': 'Chen', 'start': 1871, 'end': 1875}, {'entity': 'U-STAFF', 'score': np.float32(0.96326935), 'index': 441, 'word': 'ch', 'start': 1886, 'end': 1888}, {'entity': 'U-PHONE', 'score': np.float32(0.9805237), 'index': 451, 'word': '55', 'start': 1918, 'end': 1920}, {'entity': 'U-PHONE', 'score': np.float32(0.56220967), 'index': 452, 'word': '##5', 'start': 1920, 'end': 1921}, {'entity': 'B-PHONE', 'score': np.float32(0.3457628), 'index': 453, 'word': '-', 'start': 1922, 'end': 1923}, {'entity': 'L-PHONE', 'score': np.float32(0.8524575), 'index': 454, 'word': '234', 'start': 1924, 'end': 1927}, {'entity': 'I-PHONE', 'score': np.float32(0.47497025), 'index': 455, 'word': '-', 'start': 1928, 'end': 1929}, {'entity': 'L-PHONE', 'score': np.float32(0.9158466), 'index': 456, 'word': '56', 'start': 1930, 'end': 1932}]\n"
     ]
    }
   ],
   "source": [
    "print(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84b5bcfd-9904-40be-b076-f75a0d3d8c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst_of_lst_of_dict = copy.deepcopy(lst_of_ent_tags)\n",
    "# tn,tp = 0,0\n",
    "# fn,fp = 0,0\n",
    "# for i, lst in tqdm(enumerate(results)):\n",
    "#     for dictionary in lst:\n",
    "#         start_idx = dictionary['start']\n",
    "#         end_idx = dictionary ['end']\n",
    "#         doc_i = lst_of_docs[i]\n",
    "#         tok = doc_i[start_idx:end_idx+1]\n",
    "#         ent_tags = lst_of_ent_tags[i]\n",
    "#         entity = dictionary['entity']\n",
    "#         doc_tok = lst_of_tokens[i]\n",
    "#         try:\n",
    "            \n",
    "#             tok_idx = doc_tok.index(tok.strip())\n",
    "#         except ValueError as ve:\n",
    "#             fp += 1\n",
    "#             continue\n",
    "# # print(start_idx)\n",
    "# # print(end_idx)\n",
    "# # print(doc_i)\n",
    "# # print(tok)\n",
    "# # print(ent_tags)\n",
    "# # print(entity)\n",
    "# # print(doc_tok)\n",
    "# # raise ValueError(f'{start_idx} oh no') from ve\n",
    "\n",
    "#         try:\n",
    "            \n",
    "#             tag = ent_tags[tok_idx]\n",
    "#         except ValueError as ve:\n",
    "#             print(doc_tok)\n",
    "#             print(ent_tags)\n",
    "#             assert len(doc_tok) == len(ent_tags), f'{len(doc_tok)} != {len(ent_tags)}'\n",
    "#         is_pii = tag != 'O'\n",
    "#         if is_pii: \n",
    "#             tp += 1 \n",
    "#         else:\n",
    "            \n",
    "#             fp += 1\n",
    "#         # print(tag)\n",
    "#         # lst_of_lst_of_dict = \n",
    "#         lst_of_lst_of_dict[i].remove(tag)\n",
    "#     for tag in lst_of_lst_of_dict[i]: \n",
    "#         if tag == 'O':\n",
    "            \n",
    "#             tn += 1\n",
    "#         else:\n",
    "             \n",
    "#              fn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eacdfc47-fb3d-4328-a444-8fc930875c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_of_pii = []\n",
    "for tok_lst, lab_lst in zip(lst_of_tokens, lst_of_labels):\n",
    "    pii = []\n",
    "    for tok, lab in zip(tok_lst, lab_lst):\n",
    "        if lab != 14:\n",
    "            pii.append(tok)\n",
    "    lst_of_pii.append(pii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73a5eaca-8d02-4b76-b338-36566bf3eff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Christopher',\n",
       " 'Mu',\n",
       " '##rill',\n",
       " '##o',\n",
       " 'al',\n",
       " '##var',\n",
       " '##ez',\n",
       " '##ken',\n",
       " '##net',\n",
       " '##h',\n",
       " '@',\n",
       " 'g',\n",
       " '##mail',\n",
       " '.',\n",
       " 'com',\n",
       " '49',\n",
       " '##3',\n",
       " '-',\n",
       " '290',\n",
       " '-',\n",
       " '96',\n",
       " '##35',\n",
       " '49',\n",
       " '##3',\n",
       " '-',\n",
       " '290',\n",
       " '-',\n",
       " '96',\n",
       " '##35',\n",
       " 'heat',\n",
       " '##her',\n",
       " '##mart',\n",
       " '##ine',\n",
       " '##z',\n",
       " '82',\n",
       " '##23',\n",
       " 'Victoria',\n",
       " 'Row']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_of_tokenized_pii = []\n",
    "for lst in lst_of_pii:\n",
    "    tok_pii = []\n",
    "    for pii in lst: \n",
    "        tok = tokenizer.tokenize(pii)\n",
    "        if isinstance(tok, list):\n",
    "            for t in tok: \n",
    "                tok_pii.append(t)\n",
    "        else:\n",
    "            tok_pii.append(tok)\n",
    "    lst_of_tokenized_pii.append(tok_pii)\n",
    "#print(len(lst_of_tokenized_pii))\n",
    "lst_of_tokenized_pii[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "837d5d6c-c975-4a12-9a04-548bdff7ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lst_of_tokenized_pii[0])\n",
    "# print()\n",
    "# print(lst_of_tokenized_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40094d5c-9217-489c-9813-c6989a07d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(token):\n",
    "    return token.lstrip(\"##\").lstrip(\"Ä \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fbd5fe37-38d7-41b5-a9d4-894325b69ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs_copy = [Counter([normalize(tok) for tok in doc]) for doc in lst_of_tokenized_docs]\n",
    "tokenized_pii_copy = [Counter([normalize(tok) for tok in pii]) for pii in lst_of_tokenized_pii]\n",
    "\n",
    "fn = sum(sum(counter.values()) for counter in tokenized_pii_copy)\n",
    "tn = sum(sum(counter.values()) for counter in tokenized_docs_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86f959ce-8ce7-4c24-8343-16f28d839575",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp = 0, 0\n",
    "for i, (model_output, true_pii_counter) in enumerate(zip(results, tokenized_pii_copy)):\n",
    "    for entity in model_output:\n",
    "        word = normalize(entity['word'])\n",
    "\n",
    "        if true_pii_counter[word] > 0:\n",
    "            true_pii_counter[word] -= 1\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "        if tokenized_docs_copy[i][word] > 0:\n",
    "            tokenized_docs_copy[i][word] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3741393-6f85-4059-88f6-a2d83097b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 0\n",
    "for lst in lst_of_tokenized_pii:\n",
    "    fn += len(lst)  \n",
    "tn = 0\n",
    "for lst in lst_of_tokenized_docs:\n",
    "    tn += len(lst)\n",
    "tn = tn - fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f4b91eb-d282-4073-9771-c992d507f0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2299392"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['medical_consultations']) * 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "686fa8be-9446-4195-ab0c-b11ac6ba9157",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs_copy = [Counter([normalize(tok) for tok in doc]) for doc in lst_of_tokenized_docs]\n",
    "tokenized_pii_copy = [Counter([normalize(tok) for tok in pii]) for pii in lst_of_tokenized_pii]\n",
    "\n",
    "tp, fp = 0, 0\n",
    "for i, (model_output, true_pii_counter) in enumerate(zip(results, tokenized_pii_copy)):\n",
    "    for entity in model_output:\n",
    "        word = normalize(entity['word'])\n",
    "\n",
    "        if true_pii_counter[word] > 0:\n",
    "            true_pii_counter[word] -= 1\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "        if tokenized_docs_copy[i][word] > 0:\n",
    "            tokenized_docs_copy[i][word] -= 1\n",
    "\n",
    "fn = sum(sum(counter.values()) for counter in tokenized_pii_copy)\n",
    "tn = sum(sum(counter.values()) for counter in tokenized_docs_copy) - fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22b620b5-b43f-4c43-83ae-657507dc24af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 98133\n",
      "TN: 1965164\n",
      "FP: 79572\n",
      "FN: 180711\n"
     ]
    }
   ],
   "source": [
    "print(f\"TP: {tp}\\nTN: {tn}\\nFP: {fp}\\nFN: {fn}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec81494c-fe3c-4933-ae6b-428e7e8fb9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MD']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b448aa77-dc4d-45e1-a2b9-a080588feedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca979d-ffd5-4a22-bca7-e29286d40d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stackleberggametheory",
   "language": "python",
   "name": "stackleberggametheory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
