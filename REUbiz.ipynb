{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617ab392-97a4-459e-bed8-d2147918e737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import torch\n",
    "from typing import List, Dict, Tuple\n",
    "from datasets import load_dataset, Dataset\n",
    "from dotenv import load_dotenv\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "pipe = pipeline(\"token-classification\", model=\"obi/deid_bert_i2b2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41926ff-e6f2-44e4-8c79-7aa9a4657370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('healthcare_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e07d1b03-5a78-47a7-a254-7f90ff651e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mks-logic/SPY\", trust_remote_code=True, faker_random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "066d682e-d0b7-4088-a58f-3b564d84dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1312f34e-2b6d-4251-b99b-ffae978f9f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'legal_questions': ['tokens', 'trailing_whitespaces', 'labels', 'ent_tags'], 'medical_consultations': ['tokens', 'trailing_whitespaces', 'labels', 'ent_tags']}\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'bool'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "for x in dataset['medical_consultations']:\n",
    "    # print(x)\n",
    "    break\n",
    "print(dataset.column_names)\n",
    "lst_of_ent_tags = dataset['medical_consultations']['ent_tags']\n",
    "lst_of_tokens = dataset['medical_consultations']['tokens']\n",
    "lst_of_trailing_whitespaces = dataset['medical_consultations']['trailing_whitespaces']\n",
    "lst_of_labels = dataset['medical_consultations']['labels']\n",
    "print(type(lst_of_ent_tags[0][2]))\n",
    "print(type(lst_of_tokens[0][2]))\n",
    "print(type(lst_of_trailing_whitespaces[0][2]))\n",
    "print(type(lst_of_labels[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0910cdc0-a284-40d2-86fd-5d055175e198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', ': ', 'Hi', ', ', 'I ', 'am ', 'Christopher', 'Murillo', ', ', '\\n', 'I ', 'am ', 'experiencing ', 'a ', 'sharp ', 'pain ', 'in ', 'the ', 'lower ', 'right ', 'abdomen', ', ', 'occasionally ', 'radiating ', 'to ', 'my ', 'back', '. ', 'This ', 'symptom ', 'has ', 'been ', 'persistent ', 'for ', 'the ', 'past ', '4 ', 'days ', 'now', '. ', 'The ', 'pain ', 'is ', 'worsening ', 'over ', 'time', ', ', 'especially ', 'after ', 'eating ', 'or ', 'engaging ', 'in ', 'physical ', 'activities', '. ', 'I ', 'have ', 'noticed ', 'minor ', 'nausea', ', ', 'but ', 'no ', 'vomiting ', 'or ', 'bleeding', '. ', 'The ', 'pain ', 'is ', 'non', '-', 'specific', ', ', 'meaning ', 'it ', 'does', \"n't \", 'seem ', 'to ', 'be ', 'triggered ', 'by ', 'a ', 'specific ', 'food ', 'intake ', 'or ', 'any ', 'other ', 'activity', '. ', '\\n\\n', 'You ', 'can ', 'reach ', 'me ', 'at ', 'alvarezkenneth@gmail.com', 'or ', '493-290-9635', 'for ', 'any ', 'clarification ', 'or ', 'follow', '-', 'up ', 'questions', '.', '\\n\\n', 'I ', 'have ', 'a ', 'history ', 'of ', 'appendicitis ', 'which ', 'was ', 'surgically ', 'removed ', 'when ', 'I ', 'was ', '17', '. ', 'I ', 'have ', 'also ', 'had ', 'several ', 'episodes ', 'of ', 'stomach ', 'ulcers ', 'in ', 'the ', 'past', ', ', 'for ', 'which ', 'I ', 'have ', 'taken ', 'antacids ', 'and ', 'antibiotics ', 'as ', 'prescribed ', 'by ', 'my ', 'previous ', 'doctor', ', ', 'Dr. ', 'Emily ', 'Patel', ', ', 'whose ', 'contact ', 'information ', 'is ', 'available ', 'at ', 'https://facebook.com/healthcareprofessionals', '. ', '\\n\\n', 'Currently', ', ', 'I ', 'am ', 'not ', 'taking ', 'any ', 'medications', ', ', 'but ', 'I ', 'have ', 'an ', 'upcoming ', 'doctor', \"'s \", 'appointment ', 'with ', 'Dr. ', 'Sophia ', 'Lee ', 'on ', '6923421356', '. ', '\\n\\n', 'Can ', 'you ', 'please ', 'analyze ', 'my ', 'symptoms ', 'and ', 'help ', 'me ', 'understand ', 'what ', 'could ', 'be ', 'the ', 'underlying ', 'cause ', 'of ', 'this ', 'pain', '? ', 'What ', 'diagnostic ', 'tests ', 'would ', 'you ', 'recommend ', 'to ', 'confirm ', 'the ', 'diagnosis', '? ', 'Are ', 'there ', 'any ', 'lifestyle ', 'changes ', 'I ', 'can ', 'make ', 'to ', 'alleviate ', 'this ', 'discomfort', '? ', 'And ', 'finally', ', ', 'what ', 'are ', 'the ', 'treatment ', 'options ', 'available ', 'for ', 'this ', 'condition', '? ', 'I ', 'would ', 'appreciate ', 'it ', 'if ', 'you ', 'could ', 'send ', 'the ', 'response ', 'to ', 'my ', 'contact ', 'number ', '493-290-9635', 'for ', 'prompt ', 'action', '. ', '\\n\\n', 'I ', 'am ', 'willing ', 'to ', 'provide ', 'more ', 'information ', 'if ', 'required', '. ', 'You ', 'can ', 'also ', 'reach ', 'out ', 'to ', 'Dr. ', 'Michael ', 'Brown ', 'at ', 'michael.brown@sthealth.org ', 'or ', 'call ', 'him ', 'at ', '567', '-', '345', '-', '6789 ', 'for ', 'any ', 'additional ', 'information', '. ', 'My ', 'username ', 'is ', 'heathermartinez', ', ', 'and ', 'you ', 'can ', 'reach ', 'me ', 'anytime ', 'at ', '8223', 'Victoria', 'Row', 'or ', 'mail ', 'to ', 'St. ', 'Michael', \"'s \", 'Hospital', ', ', '1234 ', 'General ', 'Hospital ', 'Drive', ', ', 'Anytown', ', ', 'LA ', '70816', '. ', 'Thank ', 'you ', 'for ', 'your ', 'time ', 'and ', 'assistance', '.']\n",
      "6\n",
      "[False, True, False, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, False, False, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, False, False, False, False, False, False, True, True, True, True, True, False, False, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, False, True, False, False, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, True, True, True, False, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, False, False, True, True, True]\n",
      "[14, 14, 14, 14, 14, 14, 2, 9, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 14, 3, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 3, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 6, 14, 14, 14, 14, 14, 14, 14, 14, 4, 11, 11, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n"
     ]
    }
   ],
   "source": [
    "print(lst_of_tokens[0])\n",
    "print(lst_of_tokens[0].index(\"Christopher\"))\n",
    "print(lst_of_trailing_whitespaces[0])\n",
    "print(lst_of_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d50acc92-a6ad-4c59-8104-73b9860e29bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def reconstruct_text_from_tokens(tokens: List[str], has_trailing_space: List[bool], labels: List[int]) -> str:\n",
    "    \"\"\"\n",
    "    Reconstructs a text string from a list of tokens and a list of trailing whitespace indicators.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): List of token strings.\n",
    "        has_trailing_space (List[bool]): List of booleans where True means a space should follow the token.\n",
    "\n",
    "    Returns:\n",
    "        str: The reconstructed string with appropriate single-space separations.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the lengths of tokens and has_trailing_space do not match.\n",
    "    \"\"\"\n",
    "    if len(tokens) != len(has_trailing_space):\n",
    "        raise ValueError(\"Length of tokens and has_trailing_space must be equal.\")\n",
    "    \n",
    "    pieces = []\n",
    "    for token, has_space, label in zip(tokens, has_trailing_space, labels):\n",
    "        pieces.append(token)\n",
    "        if (label != 14):\n",
    "            pieces.append(\" \")\n",
    "\n",
    "    return \"\".join(pieces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e59c703f-c519-42e6-969a-b3be15c975e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hi, I am Christopher Murillo , \n",
      "I am experiencing a sharp pain in the lower right abdomen, occasionally radiating to my back. This symptom has been persistent for the past 4 days now. The pain is worsening over time, especially after eating or engaging in physical activities. I have noticed minor nausea, but no vomiting or bleeding. The pain is non-specific, meaning it doesn't seem to be triggered by a specific food intake or any other activity. \n",
      "\n",
      "You can reach me at alvarezkenneth@gmail.com or 493-290-9635 for any clarification or follow-up questions.\n",
      "\n",
      "I have a history of appendicitis which was surgically removed when I was 17. I have also had several episodes of stomach ulcers in the past, for which I have taken antacids and antibiotics as prescribed by my previous doctor, Dr. Emily Patel, whose contact information is available at https://facebook.com/healthcareprofessionals. \n",
      "\n",
      "Currently, I am not taking any medications, but I have an upcoming doctor's appointment with Dr. Sophia Lee on 6923421356. \n",
      "\n",
      "Can you please analyze my symptoms and help me understand what could be the underlying cause of this pain? What diagnostic tests would you recommend to confirm the diagnosis? Are there any lifestyle changes I can make to alleviate this discomfort? And finally, what are the treatment options available for this condition? I would appreciate it if you could send the response to my contact number 493-290-9635 for prompt action. \n",
      "\n",
      "I am willing to provide more information if required. You can also reach out to Dr. Michael Brown at michael.brown@sthealth.org or call him at 567-345-6789 for any additional information. My username is heathermartinez , and you can reach me anytime at 8223 Victoria Row or mail to St. Michael's Hospital, 1234 General Hospital Drive, Anytown, LA 70816. Thank you for your time and assistance.\n"
     ]
    }
   ],
   "source": [
    "t = reconstruct_text_from_tokens(lst_of_tokens[0],lst_of_trailing_whitespaces[0],lst_of_labels[0])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dab3d20-f44f-4f9f-9cc2-e66d74fd9683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ! I 'm Jacqueline Adams , reaching out for an online consultation due to a recurring issue with tooth sensitivity . It 's been going on for the past 6 months , and it 's been getting gradually worse . Sometimes it 's just a mild tingling sensation , but other times it 's a sharp pain . I 've noticed it 's triggered by eating or drinking something hot or cold . I 've had a few dental fillings in the past , but no major dental work . You can reach me back at christopher17@gmail.com if there 's any additional information required . I 've tried using toothpaste for sensitive teeth , but it does n't seem to be making a difference . My dentist prescribed me a fluoride rinse , but I have n't seen much improvement . In my free time , I usually play online games under the username millerchristina . I 'd appreciate it if you could recommend any products or services that are available at 6771 Johns Shores or can be shipped to my location . Could you please advise on what might be causing this sensitivity and what treatment options are available ? Would I need to come in for an in - person appointment or can we proceed with the consultation online ? Also , are there any over - the - counter medications or products you would recommend in the meantime to help alleviate the sensitivity ? Please share some useful resources and tips on my blog at https://hernandez-perez.org/listlogin.php . Additionally , you can reach me at 671.616.3448x736 if you need to clarify anything . Thank you for your time , and I look forward to your response . Dr. Smith 's dental office has an online consultation service available at dentalcareonline.com , you can also contact them at info@dentalcareonline.com or by calling 555 - 123 - 4567 . Their office is located at 123 Main St , Anytown , CA 12345 . For any urgent matters , please contact Dr. Smith 's assistant , Emily Chen at emily.chen@dentalcareonline.com or 555 - 234 - 5678 .\n"
     ]
    }
   ],
   "source": [
    "lst_of_docs = [\" \".join(lst)\n",
    "    for lst in lst_of_tokens\n",
    "]\n",
    "lst_of_docs = [\" \".join(doc.split())\n",
    "              for doc in lst_of_docs]\n",
    "print(lst_of_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d324c4-97aa-41a6-b205-ddc569271539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d0ebdcd-5ce3-43a7-b913-c9732a94e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', ': ', 'Hi', ', ', 'I ', 'am ', 'Christopher', 'Murillo', ', ', '\\n', 'I ', 'am ', 'experiencing ', 'a ', 'sharp ', 'pain ', 'in ', 'the ', 'lower ', 'right ', 'abdomen', ', ', 'occasionally ', 'radiating ', 'to ', 'my ', 'back', '. ', 'This ', 'symptom ', 'has ', 'been ', 'persistent ', 'for ', 'the ', 'past ', '4 ', 'days ', 'now', '. ', 'The ', 'pain ', 'is ', 'worsening ', 'over ', 'time', ', ', 'especially ', 'after ', 'eating ', 'or ', 'engaging ', 'in ', 'physical ', 'activities', '. ', 'I ', 'have ', 'noticed ', 'minor ', 'nausea', ', ', 'but ', 'no ', 'vomiting ', 'or ', 'bleeding', '. ', 'The ', 'pain ', 'is ', 'non', '-', 'specific', ', ', 'meaning ', 'it ', 'does', \"n't \", 'seem ', 'to ', 'be ', 'triggered ', 'by ', 'a ', 'specific ', 'food ', 'intake ', 'or ', 'any ', 'other ', 'activity', '. ', '\\n\\n', 'You ', 'can ', 'reach ', 'me ', 'at ', 'alvarezkenneth@gmail.com', 'or ', '493-290-9635', 'for ', 'any ', 'clarification ', 'or ', 'follow', '-', 'up ', 'questions', '.', '\\n\\n', 'I ', 'have ', 'a ', 'history ', 'of ', 'appendicitis ', 'which ', 'was ', 'surgically ', 'removed ', 'when ', 'I ', 'was ', '17', '. ', 'I ', 'have ', 'also ', 'had ', 'several ', 'episodes ', 'of ', 'stomach ', 'ulcers ', 'in ', 'the ', 'past', ', ', 'for ', 'which ', 'I ', 'have ', 'taken ', 'antacids ', 'and ', 'antibiotics ', 'as ', 'prescribed ', 'by ', 'my ', 'previous ', 'doctor', ', ', 'Dr. ', 'Emily ', 'Patel', ', ', 'whose ', 'contact ', 'information ', 'is ', 'available ', 'at ', 'https://facebook.com/healthcareprofessionals', '. ', '\\n\\n', 'Currently', ', ', 'I ', 'am ', 'not ', 'taking ', 'any ', 'medications', ', ', 'but ', 'I ', 'have ', 'an ', 'upcoming ', 'doctor', \"'s \", 'appointment ', 'with ', 'Dr. ', 'Sophia ', 'Lee ', 'on ', '6923421356', '. ', '\\n\\n', 'Can ', 'you ', 'please ', 'analyze ', 'my ', 'symptoms ', 'and ', 'help ', 'me ', 'understand ', 'what ', 'could ', 'be ', 'the ', 'underlying ', 'cause ', 'of ', 'this ', 'pain', '? ', 'What ', 'diagnostic ', 'tests ', 'would ', 'you ', 'recommend ', 'to ', 'confirm ', 'the ', 'diagnosis', '? ', 'Are ', 'there ', 'any ', 'lifestyle ', 'changes ', 'I ', 'can ', 'make ', 'to ', 'alleviate ', 'this ', 'discomfort', '? ', 'And ', 'finally', ', ', 'what ', 'are ', 'the ', 'treatment ', 'options ', 'available ', 'for ', 'this ', 'condition', '? ', 'I ', 'would ', 'appreciate ', 'it ', 'if ', 'you ', 'could ', 'send ', 'the ', 'response ', 'to ', 'my ', 'contact ', 'number ', '493-290-9635', 'for ', 'prompt ', 'action', '. ', '\\n\\n', 'I ', 'am ', 'willing ', 'to ', 'provide ', 'more ', 'information ', 'if ', 'required', '. ', 'You ', 'can ', 'also ', 'reach ', 'out ', 'to ', 'Dr. ', 'Michael ', 'Brown ', 'at ', 'michael.brown@sthealth.org ', 'or ', 'call ', 'him ', 'at ', '567', '-', '345', '-', '6789 ', 'for ', 'any ', 'additional ', 'information', '. ', 'My ', 'username ', 'is ', 'heathermartinez', ', ', 'and ', 'you ', 'can ', 'reach ', 'me ', 'anytime ', 'at ', '8223', 'Victoria', 'Row', 'or ', 'mail ', 'to ', 'St. ', 'Michael', \"'s \", 'Hospital', ', ', '1234 ', 'General ', 'Hospital ', 'Drive', ', ', 'Anytown', ', ', 'LA ', '70816', '. ', 'Thank ', 'you ', 'for ', 'your ', 'time ', 'and ', 'assistance', '.']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['medical_consultations']['tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a7385da-95af-4123-b8c9-cc257ba669fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset['medical_consultations'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f44b60b-812b-453b-9fca-d57ee80f88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['tokens'].apply(lambda x: \"\".join(x))\n",
    "dataset_txt=Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d817789-f3ee-4422-980e-e93ce61f0d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset_txt['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3925a520-1777-40bf-a05a-2d9a0dd9387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "#from transformers import pipeline\n",
    "\n",
    "#pipe = pipeline(\"token-classification\", model=\"obi/deid_roberta_i2b2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2bd71a3-1183-41e9-92b3-6dd42dd2269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results= pipe{dataset['medical_consultations']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28af1229-b997-4543-830b-b57ca62fa58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['medical_consultations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab825afe-2680-465a-87ef-c72ba27609a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results=[]\n",
    "#for out in pipe(dataset['medical_consultations']):\n",
    "    \n",
    "    #results.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3aea7e2-bbce-4c5c-8a0a-c9d91a6ba2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_v2 = dataset_txt['text'].map(\n",
    "          #  pipe,\n",
    "           # batched=True,\n",
    "            #batch_size=4,\n",
    "        # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f407bad-c975-4703-8776-e92d3112569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe(['my name is president Barack Obama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "570b09d0-d832-4f04-b284-ea4e5f7cab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results=[]\n",
    "# #with torch.inference_mode():\n",
    "# for inp in dataset_txt['text']:\n",
    "#     # print(inp)\n",
    "#     assert isinstance(inp, str), \"not a str\"\n",
    "#     out=pipe([inp], truncation=True)\n",
    "#     results.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78841311-a5e1-4684-8561-70c13fb25085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"obi/deid_roberta_i2b2\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"obi/deid_roberta_i2b2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6d7d50e-0f5c-4098-879d-8aa2c80b58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(inp, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6fd1aef2-0ecc-47a1-8720-b86f6c54b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ac09766-2679-4820-acdc-78adb2237486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs.logits\n",
    "# tokenizer.decode(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ab8d80c-1b2f-47aa-931e-c332dbc9e2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2d737121a846d586466646f07a4146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = pipe.tokenizer  # get tokenizer from your existing pipeline\n",
    "\n",
    "results = []\n",
    "for inp in tqdm(lst_of_docs):\n",
    "    assert isinstance(inp, str), \"not a str\"\n",
    "    \n",
    "    # Truncate the input at token level\n",
    "    inputs = tokenizer(\n",
    "        inp,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    # Decode back to string after truncation\n",
    "    truncated_text = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "    # Pass the truncated string to the pipeline\n",
    "    out = pipe(truncated_text)\n",
    "    results.append(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14889b73-8471-453d-8155-d2fe9d1ee59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06433caa-8f48-421e-a258-85636035971d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d511c600-6b47-47e2-952a-59a25e718858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_entities(tokens, ner_tags):\n",
    "    \"\"\"\n",
    "    this function does something\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): labeled tokens\n",
    "        ner_tags (List[str]) Maybe ground truth labels? but idk\n",
    "\n",
    "    Returns:\n",
    "        entities (List[dict[str, str])): ????\n",
    "    \"\"\"\n",
    "    # label_map = dataset['medical_consultations'].features['ent_tags'].feature.names\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    for token, tag in zip(tokens, ner_tags):\n",
    "        # tag_label = label_map[tag]\n",
    "        if tag_label.startswith(\"B-\"):\n",
    "            if current_entity:\n",
    "                entities.append(\" \".join(current_entity))\n",
    "            current_entity = [token]\n",
    "        elif tag_label.startswith(\"I-\") and current_entity:\n",
    "            current_entity.append(token)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entities.append(\" \".join(current_entity))\n",
    "                current_entity = []\n",
    "    if current_entity:\n",
    "        entities.append(\" \".join(current_entity))\n",
    "    return entities\n",
    "\n",
    "# results3 = pipe(truncated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af6a131c-e5c3-409b-8e23-22e031d95cba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lst_of_ent_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m get_true_entities(lst_of_ent_tags, results)\n",
      "\u001b[31mNameError\u001b[39m: name 'lst_of_ent_tags' is not defined"
     ]
    }
   ],
   "source": [
    "get_true_entities(lst_of_ent_tags, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e27ea26-de7f-48ac-a753-1af4e30a6823",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m lst_of_lst_of_dict = copy.deepcopy(lst_of_ent_tags)\n\u001b[32m      2\u001b[39m tn,tp = \u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m\n\u001b[32m      3\u001b[39m fn,fp = \u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'copy' is not defined"
     ]
    }
   ],
   "source": [
    "lst_of_lst_of_dict = copy.deepcopy(lst_of_ent_tags)\n",
    "tn,tp = 0,0\n",
    "fn,fp = 0,0\n",
    "for i, lst in tqdm(enumerate(results)):\n",
    "    for dictionary in lst:\n",
    "        start_idx = dictionary['start']\n",
    "        end_idx = dictionary ['end']\n",
    "        doc_i = lst_of_docs[i]\n",
    "        tok = doc_i[start_idx:end_idx+1]\n",
    "        ent_tags = lst_of_ent_tags[i]\n",
    "        entity = dictionary['start']\n",
    "        doc_tok = lst_of_tokens[i]\n",
    "        try:\n",
    "            \n",
    "            tok_idx = doc_tok.index(tok.strip())\n",
    "        except ValueError as ve:\n",
    "            fp += 1\n",
    "            continue\n",
    "# print(start_idx)\n",
    "# print(end_idx)\n",
    "# print(doc_i)\n",
    "# print(tok)\n",
    "# print(ent_tags)\n",
    "# print(entity)\n",
    "# print(doc_tok)\n",
    "# raise ValueError(f'{start_idx} oh no') from ve\n",
    "\n",
    "        try:\n",
    "            \n",
    "            tag = ent_tags[tok_idx]\n",
    "        except ValueError as ve:\n",
    "            print(doc_tok)\n",
    "            print(ent_tags)\n",
    "            assert len(doc_tok) == len(ent_tags), f'{len(doc_tok)} != {len(ent_tags)}'\n",
    "        is_pii = tag != 'O'\n",
    "        if is_pii: \n",
    "            tp += 1 \n",
    "        else:\n",
    "            \n",
    "            fp += 1\n",
    "        # print(tag)\n",
    "        # lst_of_lst_of_dict = \n",
    "        lst_of_lst_of_dict[i].remove(tag)\n",
    "    for tag in lst_of_lst_of_dict[i]: \n",
    "        if tag == 'O':\n",
    "            \n",
    "            tn += 1\n",
    "        else:\n",
    "             \n",
    "             fn += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b55893-b659-4b76-85c3-435902202107",
   "metadata": {},
   "source": [
    "## new code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc95394c-09aa-49e5-8665-134b5289bf06",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m lst_of_lst_of_dict = copy.deepcopy(lst_of_ent_tags)\n\u001b[32m      2\u001b[39m tn,tp = \u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m\n\u001b[32m      3\u001b[39m fn,fp = \u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'copy' is not defined"
     ]
    }
   ],
   "source": [
    "lst_of_lst_of_dict = copy.deepcopy(lst_of_ent_tags)\n",
    "tn,tp = 0,0\n",
    "fn,fp = 0,0\n",
    "for i, lst in tqdm(enumerate(results)):\n",
    "    for dictionary in lst:\n",
    "        start_idx = dictionary['start']\n",
    "        end_idx = dictionary ['end']\n",
    "        doc_i = lst_of_docs[i]\n",
    "        tok = doc_i[start_idx:end_idx+1]\n",
    "        ent_tags = lst_of_ent_tags[i]\n",
    "        entity = dictionary['start']\n",
    "        doc_tok = lst_of_tokens[i]\n",
    "        try:\n",
    "            \n",
    "            tok_idx = doc_tok.index(tok.strip())\n",
    "        except ValueError as ve:\n",
    "            fp += 1\n",
    "            continue\n",
    "# print(start_idx)\n",
    "# print(end_idx)\n",
    "# print(doc_i)\n",
    "# print(tok)\n",
    "# print(ent_tags)\n",
    "# print(entity)\n",
    "# print(doc_tok)\n",
    "# raise ValueError(f'{start_idx} oh no') from ve\n",
    "\n",
    "        try:\n",
    "            \n",
    "            tag = ent_tags[tok_idx]\n",
    "        except ValueError as ve:\n",
    "            print(doc_tok)\n",
    "            print(ent_tags)\n",
    "            assert len(doc_tok) == len(ent_tags), f'{len(doc_tok)} != {len(ent_tags)}'\n",
    "        is_pii = tag != 'O'\n",
    "        if is_pii: \n",
    "            tp += 1 \n",
    "        else:\n",
    "            \n",
    "            fp += 1\n",
    "        # print(tag)\n",
    "        # lst_of_lst_of_dict = \n",
    "        lst_of_lst_of_dict[i].remove(tag)\n",
    "    for tag in lst_of_lst_of_dict[i]: \n",
    "        if tag == 'O':\n",
    "            \n",
    "            tn += 1\n",
    "        else:\n",
    "             \n",
    "             fn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a07032-5c40-4b44-b046-5a5121afdeb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m i = \u001b[32m0\u001b[39m\n\u001b[32m      2\u001b[39m tp, fp = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lst_of_pii, pii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, lst_of_tokenized_pii):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m dictionary \u001b[38;5;129;01min\u001b[39;00m lst_of_pii:\n\u001b[32m      5\u001b[39m         word = dictionary[\u001b[33m'\u001b[39m\u001b[33mword\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "tp, fp = 0, 0\n",
    "for lst_of_pii, pii in zip(results, lst_of_tokenized_pii):\n",
    "    for dictionary in lst_of_pii:\n",
    "        word = dictionary['word']\n",
    "        if word in pii:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1 \n",
    "        lst_of_tokenized_docs[i].remove(word)\n",
    "    i += 1\n",
    "print(f\"TP: {tp}\\nFP: {fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88722889-9e48-4175-a85c-b1f206449d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def reconstruct_text_from_tokens(tokens: List[str], has_trailing_space: List[bool], labels: List[int]) -> str:\n",
    "    \"\"\"\n",
    "    Reconstructs a text string from a list of tokens and a list of trailing whitespace indicators.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): List of token strings.\n",
    "        has_trailing_space (List[bool]): List of booleans where True means a space should follow the token.\n",
    "\n",
    "    Returns:\n",
    "        str: The reconstructed string with appropriate single-space separations.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the lengths of tokens and has_trailing_space do not match.\n",
    "    \"\"\"\n",
    "    if len(tokens) != len(has_trailing_space):\n",
    "        raise ValueError(\"Length of tokens and has_trailing_space must be equal.\")\n",
    "    \n",
    "    pieces = []\n",
    "    for token, has_space, label in zip(tokens, has_trailing_space, labels):\n",
    "        pieces.append(token)\n",
    "        if (label != 14):\n",
    "            pieces.append(\" \")\n",
    "\n",
    "    return \"\".join(pieces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a480e9b4-5755-43f6-9e6a-1e92646cb9bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lst_of_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m lst_of_docs = []\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tok_lst, space_lst, label_lst \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lst_of_tokens, lst_of_trailing_whitespaces, lst_of_labels):\n\u001b[32m      3\u001b[39m     doc = reconstruct_text_from_tokens(tok_lst, space_lst, label_lst)\n\u001b[32m      4\u001b[39m     lst_of_docs.append(doc)\n",
      "\u001b[31mNameError\u001b[39m: name 'lst_of_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "lst_of_docs = []\n",
    "for tok_lst, space_lst, label_lst in zip(lst_of_tokens, lst_of_trailing_whitespaces, lst_of_labels):\n",
    "    doc = reconstruct_text_from_tokens(tok_lst, space_lst, label_lst)\n",
    "    lst_of_docs.append(doc)\n",
    "lst_of_tokenized_docs = []\n",
    "for doc in lst_of_docs:\n",
    "    tokenized_doc = tokenizer.tokenize(doc)\n",
    "    if isinstance(tokenized_doc, list):\n",
    "        lst_of_tokenized_docs.append(tokenized_doc)\n",
    "    else:\n",
    "        lst_of_tokenized_docs.append([tokenized_doc])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21711a04-66d7-4c01-a570-49645f513af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 0\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"TP: {tp}\\nTN: {tn}\\nFP: {fp}\\nFN: {fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f24cd3e3-13a1-4815-be01-14c375a3ba26",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lst_of_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m lst_of_pii = []\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tok_lst, lab_lst \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lst_of_tokens, lst_of_labels):\n\u001b[32m      3\u001b[39m     pii = []\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m tok, lab \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tok_lst, lab_lst):\n",
      "\u001b[31mNameError\u001b[39m: name 'lst_of_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "lst_of_pii = []\n",
    "for tok_lst, lab_lst in zip(lst_of_tokens, lst_of_labels):\n",
    "    pii = []\n",
    "    for tok, lab in zip(tok_lst, lab_lst):\n",
    "        if lab != 14:\n",
    "            pii.append(tok)\n",
    "    lst_of_pii.append(pii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4063d537-4ddc-465f-8712-864aa56f056c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m     lst_of_tokenized_pii.append(tok_pii)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lst_of_tokenized_pii))\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m lst_of_tokenized_pii[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "lst_of_tokenized_pii = []\n",
    "for lst in lst_of_pii:\n",
    "    tok_pii = []\n",
    "    for pii in lst: \n",
    "        tok = tokenizer.tokenize(pii)\n",
    "        if isinstance(tok, list):\n",
    "            for t in tok: \n",
    "                tok_pii.append(t)\n",
    "        else:\n",
    "            tok_pii.append(tok)\n",
    "    lst_of_tokenized_pii.append(tok_pii)\n",
    "print(len(lst_of_tokenized_pii))\n",
    "lst_of_tokenized_pii[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d3b9063-b281-40db-bd9e-93b440466a81",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m i = \u001b[32m0\u001b[39m \n\u001b[32m      2\u001b[39m fn,tn = \u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lst \u001b[38;5;129;01min\u001b[39;00m copy.deepcopy(lst_of_tokenized_pii): \n\u001b[32m      4\u001b[39m     fn += \u001b[38;5;28mlen\u001b[39m(lst)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m lst:\n",
      "\u001b[31mNameError\u001b[39m: name 'copy' is not defined"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "fn,tn = 0,0\n",
    "for lst in copy.deepcopy(lst_of_tokenized_pii): \n",
    "    fn += len(lst)\n",
    "    for tok in lst:\n",
    "        lst_of_tokenized_docs[i].remove(tok)\n",
    "    i += 1 \n",
    "for lst in lst_of_tokenized_docs:\n",
    "    tn += len(lst)\n",
    "\n",
    "print(f\"FN: {fn}\\nTN: {tn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13cfe550-38c8-49f1-9644-a727f9ccc39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lst_of_pii[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f1e17-039a-4a29-9b0d-57ae15b8571d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f1e39-b8fb-40e0-80c9-91d665e6eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entity_char_spans(tokens: List[str], trailing_spaces: List[bool]) -> List[Tuple[int, int, str]]:\n",
    "    \"\"\"\n",
    "    Computes the character-level spans for each token and its entity tag.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): List of tokens in the original input.\n",
    "        trailing_spaces (List[bool]): Booleans indicating whether a space follows each token.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[int, int, str]]: List of (start_char, end_char, token) tuples for each token.\n",
    "    \"\"\"\n",
    "    spans = []\n",
    "    current_position = 0\n",
    "    for token, has_space in zip(tokens, trailing_spaces):\n",
    "        start = current_position\n",
    "        end = start + len(token)\n",
    "        spans.append((start, end, token))\n",
    "        current_position = end + (1 if has_space else 0)\n",
    "    return spans\n",
    "\n",
    "\n",
    "def evaluate_predictions(\n",
    "    results: List[List[Dict]],\n",
    "    tokens_list: List[List[str]],\n",
    "    whitespace_list: List[List[bool]],\n",
    "    ent_tags_list: List[List[str]]\n",
    ") -> Tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Evaluates model predictions against ground truth using character span matching.\n",
    "\n",
    "    Args:\n",
    "        results (List[List[Dict]]): Model output with predicted PII spans.\n",
    "        tokens_list (List[List[str]]): List of token sequences per document.\n",
    "        whitespace_list (List[List[bool]]): List of trailing space indicators per document.\n",
    "        ent_tags_list (List[List[str]]): Ground truth PII tags per document.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, int, int, int]: (TP, FP, TN, FN) counts.\n",
    "    \"\"\"\n",
    "    tp = fp = tn = fn = 0\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        tokens = tokens_list[i]\n",
    "        trailing_spaces = whitespace_list[i]\n",
    "        ent_tags = ent_tags_list[i]\n",
    "\n",
    "        token_spans = compute_entity_char_spans(tokens, trailing_spaces)\n",
    "\n",
    "        # Ground truth spans\n",
    "        gt_spans = set()\n",
    "        for tag, span in zip(ent_tags, token_spans):\n",
    "            if tag != \"O\":\n",
    "                gt_spans.add(span)\n",
    "\n",
    "        # Predicted spans\n",
    "        pred_spans = set()\n",
    "        for entity in result:\n",
    "            pred_spans.add((entity[\"start\"], entity[\"end\"]))\n",
    "\n",
    "        # Count TP, FP, FN\n",
    "        tp += len(pred_spans & gt_spans)\n",
    "        fp += len(pred_spans - gt_spans)\n",
    "        fn += len(gt_spans - pred_spans)\n",
    "\n",
    "        # Count TN (tokens that are neither predicted nor PII)\n",
    "        total_tokens = len(tokens)\n",
    "        pred_token_idxs = {\n",
    "            i for i, span in enumerate(token_spans) if span in pred_spans\n",
    "        }\n",
    "        gt_token_idxs = {\n",
    "            i for i, span in enumerate(token_spans) if span in gt_spans\n",
    "        }\n",
    "        for i in range(total_tokens):\n",
    "            if i not in pred_token_idxs and i not in gt_token_idxs:\n",
    "                tn += 1\n",
    "\n",
    "    return tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de09d17-cf87-43b0-a897-4506dd3e0759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp, fp, tn, fn = evaluate_predictions(\n",
    "#     results=results,\n",
    "#     tokens_list=lst_of_tokens,\n",
    "#     whitespace_list=lst_of_trailing_whitespaces,\n",
    "#     ent_tags_list=lst_of_ent_tags\n",
    "# )\n",
    "\n",
    "# print(f\"True Positives: {tp}\")\n",
    "# print(f\"False Positives: {fp}\")\n",
    "# print(f\"True Negatives: {tn}\")\n",
    "# print(f\"False Negatives: {fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11862d9-6943-4579-8484-5ff7a26211aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_indices_from_predictions(\n",
    "    model_outputs: list[dict], text: str, tokens: list[str], whitespaces: list[str]\n",
    ") -> set[int]:\n",
    "    \"\"\"\n",
    "    Maps character-level model predictions to token indices.\n",
    "\n",
    "    Args:\n",
    "        model_outputs: List of model entity predictions for one input.\n",
    "        text: Original text string used as input to the model.\n",
    "        tokens: Tokenized version of the text.\n",
    "        whitespaces: List of trailing whitespace after each token.\n",
    "\n",
    "    Returns:\n",
    "        A set of token indices that were predicted as entities.\n",
    "    \"\"\"\n",
    "    char_spans = compute_entity_char_spans(tokens, whitespaces)\n",
    "    predicted_indices = set()\n",
    "\n",
    "    for pred in model_outputs:\n",
    "        pred_start = pred[\"start\"]\n",
    "        pred_end = pred[\"end\"]\n",
    "\n",
    "        for idx, (start, end, _) in enumerate(char_spans):\n",
    "            if not (pred_end <= start or pred_start >= end):  # Overlap\n",
    "                predicted_indices.add(idx)\n",
    "\n",
    "    return predicted_indices\n",
    "\n",
    "\n",
    "def get_token_indices_from_ground_truth(ent_tags: list[str]) -> set[int]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        ent_tags: List of entity tags for each token.\n",
    "\n",
    "    Returns:\n",
    "        Set of indices of tokens labeled as PII.\n",
    "    \"\"\"\n",
    "    return {i for i, tag in enumerate(ent_tags) if tag != \"O\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c48bfd-9c0e-4feb-90c9-e4ab063e8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_token_level(results, lst_of_tokens, lst_of_trailing_ws, lst_of_ent_tags):\n",
    "    TP = FP = FN = TN = 0\n",
    "\n",
    "    for preds, tokens, ws, labels in zip(results, lst_of_tokens, lst_of_trailing_ws, lst_of_ent_tags):\n",
    "        text = reconstruct_text_from_tokens(tokens, ws, labels)  # Should be consistent with model input\n",
    "        pred_set = get_token_indices_from_predictions(preds, text, tokens, ws)\n",
    "        gold_set = get_token_indices_from_ground_truth(labels)\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            if i in pred_set and i in gold_set:\n",
    "                TP += 1\n",
    "            elif i in pred_set and i not in gold_set:\n",
    "                FP += 1\n",
    "            elif i not in pred_set and i in gold_set:\n",
    "                FN += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "\n",
    "    return {\"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415c1a0-2d20-4dac-8284-dbeef47025b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stackleberggametheory",
   "language": "python",
   "name": "stackleberggametheory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
