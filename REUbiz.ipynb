{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "617ab392-97a4-459e-bed8-d2147918e737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from dotenv import load_dotenv\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "pipe = pipeline(\"token-classification\", model=\"obi/deid_bert_i2b2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d41926ff-e6f2-44e4-8c79-7aa9a4657370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('healthcare_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e07d1b03-5a78-47a7-a254-7f90ff651e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mks-logic/SPY\", trust_remote_code=True, faker_random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "066d682e-d0b7-4088-a58f-3b564d84dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1312f34e-2b6d-4251-b99b-ffae978f9f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'legal_questions': ['tokens', 'trailing_whitespaces', 'labels', 'ent_tags'], 'medical_consultations': ['tokens', 'trailing_whitespaces', 'labels', 'ent_tags']}\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'bool'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "for x in dataset['medical_consultations']:\n",
    "    # print(x)\n",
    "    break\n",
    "print(dataset.column_names)\n",
    "lst_of_ent_tags = dataset['medical_consultations']['ent_tags']\n",
    "lst_of_tokens = dataset['medical_consultations']['tokens']\n",
    "lst_of_trailing_whitespaces = dataset['medical_consultations']['trailing_whitespaces']\n",
    "lst_of_labels = dataset['medical_consultations']['labels']\n",
    "print(type(lst_of_ent_tags[0][2]))\n",
    "print(type(lst_of_tokens[0][2]))\n",
    "print(type(lst_of_trailing_whitespaces[0][2]))\n",
    "print(type(lst_of_labels[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0910cdc0-a284-40d2-86fd-5d055175e198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', ': ', 'Hi', ', ', 'I ', 'am ', 'Christopher', 'Murillo', ', ', '\\n', 'I ', 'am ', 'experiencing ', 'a ', 'sharp ', 'pain ', 'in ', 'the ', 'lower ', 'right ', 'abdomen', ', ', 'occasionally ', 'radiating ', 'to ', 'my ', 'back', '. ', 'This ', 'symptom ', 'has ', 'been ', 'persistent ', 'for ', 'the ', 'past ', '4 ', 'days ', 'now', '. ', 'The ', 'pain ', 'is ', 'worsening ', 'over ', 'time', ', ', 'especially ', 'after ', 'eating ', 'or ', 'engaging ', 'in ', 'physical ', 'activities', '. ', 'I ', 'have ', 'noticed ', 'minor ', 'nausea', ', ', 'but ', 'no ', 'vomiting ', 'or ', 'bleeding', '. ', 'The ', 'pain ', 'is ', 'non', '-', 'specific', ', ', 'meaning ', 'it ', 'does', \"n't \", 'seem ', 'to ', 'be ', 'triggered ', 'by ', 'a ', 'specific ', 'food ', 'intake ', 'or ', 'any ', 'other ', 'activity', '. ', '\\n\\n', 'You ', 'can ', 'reach ', 'me ', 'at ', 'alvarezkenneth@gmail.com', 'or ', '493-290-9635', 'for ', 'any ', 'clarification ', 'or ', 'follow', '-', 'up ', 'questions', '.', '\\n\\n', 'I ', 'have ', 'a ', 'history ', 'of ', 'appendicitis ', 'which ', 'was ', 'surgically ', 'removed ', 'when ', 'I ', 'was ', '17', '. ', 'I ', 'have ', 'also ', 'had ', 'several ', 'episodes ', 'of ', 'stomach ', 'ulcers ', 'in ', 'the ', 'past', ', ', 'for ', 'which ', 'I ', 'have ', 'taken ', 'antacids ', 'and ', 'antibiotics ', 'as ', 'prescribed ', 'by ', 'my ', 'previous ', 'doctor', ', ', 'Dr. ', 'Emily ', 'Patel', ', ', 'whose ', 'contact ', 'information ', 'is ', 'available ', 'at ', 'https://facebook.com/healthcareprofessionals', '. ', '\\n\\n', 'Currently', ', ', 'I ', 'am ', 'not ', 'taking ', 'any ', 'medications', ', ', 'but ', 'I ', 'have ', 'an ', 'upcoming ', 'doctor', \"'s \", 'appointment ', 'with ', 'Dr. ', 'Sophia ', 'Lee ', 'on ', '6923421356', '. ', '\\n\\n', 'Can ', 'you ', 'please ', 'analyze ', 'my ', 'symptoms ', 'and ', 'help ', 'me ', 'understand ', 'what ', 'could ', 'be ', 'the ', 'underlying ', 'cause ', 'of ', 'this ', 'pain', '? ', 'What ', 'diagnostic ', 'tests ', 'would ', 'you ', 'recommend ', 'to ', 'confirm ', 'the ', 'diagnosis', '? ', 'Are ', 'there ', 'any ', 'lifestyle ', 'changes ', 'I ', 'can ', 'make ', 'to ', 'alleviate ', 'this ', 'discomfort', '? ', 'And ', 'finally', ', ', 'what ', 'are ', 'the ', 'treatment ', 'options ', 'available ', 'for ', 'this ', 'condition', '? ', 'I ', 'would ', 'appreciate ', 'it ', 'if ', 'you ', 'could ', 'send ', 'the ', 'response ', 'to ', 'my ', 'contact ', 'number ', '493-290-9635', 'for ', 'prompt ', 'action', '. ', '\\n\\n', 'I ', 'am ', 'willing ', 'to ', 'provide ', 'more ', 'information ', 'if ', 'required', '. ', 'You ', 'can ', 'also ', 'reach ', 'out ', 'to ', 'Dr. ', 'Michael ', 'Brown ', 'at ', 'michael.brown@sthealth.org ', 'or ', 'call ', 'him ', 'at ', '567', '-', '345', '-', '6789 ', 'for ', 'any ', 'additional ', 'information', '. ', 'My ', 'username ', 'is ', 'heathermartinez', ', ', 'and ', 'you ', 'can ', 'reach ', 'me ', 'anytime ', 'at ', '8223', 'Victoria', 'Row', 'or ', 'mail ', 'to ', 'St. ', 'Michael', \"'s \", 'Hospital', ', ', '1234 ', 'General ', 'Hospital ', 'Drive', ', ', 'Anytown', ', ', 'LA ', '70816', '. ', 'Thank ', 'you ', 'for ', 'your ', 'time ', 'and ', 'assistance', '.']\n",
      "6\n",
      "[False, True, False, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, False, False, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, False, False, False, False, False, False, True, True, True, True, True, False, False, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, False, True, False, False, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, True, True, True, False, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, False, False, True, True, True]\n",
      "[14, 14, 14, 14, 14, 14, 2, 9, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 14, 3, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 3, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 6, 14, 14, 14, 14, 14, 14, 14, 14, 4, 11, 11, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n"
     ]
    }
   ],
   "source": [
    "print(lst_of_tokens[0])\n",
    "print(lst_of_tokens[0].index(\"Christopher\"))\n",
    "print(lst_of_trailing_whitespaces[0])\n",
    "print(lst_of_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d50acc92-a6ad-4c59-8104-73b9860e29bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def reconstruct_text_from_tokens(tokens: List[str], has_trailing_space: List[bool], labels: List[int]) -> str:\n",
    "    \"\"\"\n",
    "    Reconstructs a text string from a list of tokens and a list of trailing whitespace indicators.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): List of token strings.\n",
    "        has_trailing_space (List[bool]): List of booleans where True means a space should follow the token.\n",
    "\n",
    "    Returns:\n",
    "        str: The reconstructed string with appropriate single-space separations.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the lengths of tokens and has_trailing_space do not match.\n",
    "    \"\"\"\n",
    "    if len(tokens) != len(has_trailing_space):\n",
    "        raise ValueError(\"Length of tokens and has_trailing_space must be equal.\")\n",
    "    \n",
    "    pieces = []\n",
    "    for token, has_space, label in zip(tokens, has_trailing_space, labels):\n",
    "        pieces.append(token)\n",
    "        if (label != 14):\n",
    "            pieces.append(\" \")\n",
    "\n",
    "    return \"\".join(pieces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e59c703f-c519-42e6-969a-b3be15c975e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hi, I am Christopher Murillo , \n",
      "I am experiencing a sharp pain in the lower right abdomen, occasionally radiating to my back. This symptom has been persistent for the past 4 days now. The pain is worsening over time, especially after eating or engaging in physical activities. I have noticed minor nausea, but no vomiting or bleeding. The pain is non-specific, meaning it doesn't seem to be triggered by a specific food intake or any other activity. \n",
      "\n",
      "You can reach me at alvarezkenneth@gmail.com or 493-290-9635 for any clarification or follow-up questions.\n",
      "\n",
      "I have a history of appendicitis which was surgically removed when I was 17. I have also had several episodes of stomach ulcers in the past, for which I have taken antacids and antibiotics as prescribed by my previous doctor, Dr. Emily Patel, whose contact information is available at https://facebook.com/healthcareprofessionals. \n",
      "\n",
      "Currently, I am not taking any medications, but I have an upcoming doctor's appointment with Dr. Sophia Lee on 6923421356. \n",
      "\n",
      "Can you please analyze my symptoms and help me understand what could be the underlying cause of this pain? What diagnostic tests would you recommend to confirm the diagnosis? Are there any lifestyle changes I can make to alleviate this discomfort? And finally, what are the treatment options available for this condition? I would appreciate it if you could send the response to my contact number 493-290-9635 for prompt action. \n",
      "\n",
      "I am willing to provide more information if required. You can also reach out to Dr. Michael Brown at michael.brown@sthealth.org or call him at 567-345-6789 for any additional information. My username is heathermartinez , and you can reach me anytime at 8223 Victoria Row or mail to St. Michael's Hospital, 1234 General Hospital Drive, Anytown, LA 70816. Thank you for your time and assistance.\n"
     ]
    }
   ],
   "source": [
    "t = reconstruct_text_from_tokens(lst_of_tokens[0],lst_of_trailing_whitespaces[0],lst_of_labels[0])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dab3d20-f44f-4f9f-9cc2-e66d74fd9683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ! I 'm Jacqueline Adams , reaching out for an online consultation due to a recurring issue with tooth sensitivity . It 's been going on for the past 6 months , and it 's been getting gradually worse . Sometimes it 's just a mild tingling sensation , but other times it 's a sharp pain . I 've noticed it 's triggered by eating or drinking something hot or cold . I 've had a few dental fillings in the past , but no major dental work . You can reach me back at christopher17@gmail.com if there 's any additional information required . I 've tried using toothpaste for sensitive teeth , but it does n't seem to be making a difference . My dentist prescribed me a fluoride rinse , but I have n't seen much improvement . In my free time , I usually play online games under the username millerchristina . I 'd appreciate it if you could recommend any products or services that are available at 6771 Johns Shores or can be shipped to my location . Could you please advise on what might be causing this sensitivity and what treatment options are available ? Would I need to come in for an in - person appointment or can we proceed with the consultation online ? Also , are there any over - the - counter medications or products you would recommend in the meantime to help alleviate the sensitivity ? Please share some useful resources and tips on my blog at https://hernandez-perez.org/listlogin.php . Additionally , you can reach me at 671.616.3448x736 if you need to clarify anything . Thank you for your time , and I look forward to your response . Dr. Smith 's dental office has an online consultation service available at dentalcareonline.com , you can also contact them at info@dentalcareonline.com or by calling 555 - 123 - 4567 . Their office is located at 123 Main St , Anytown , CA 12345 . For any urgent matters , please contact Dr. Smith 's assistant , Emily Chen at emily.chen@dentalcareonline.com or 555 - 234 - 5678 .\n"
     ]
    }
   ],
   "source": [
    "lst_of_docs = [\" \".join(lst)\n",
    "    for lst in lst_of_tokens\n",
    "]\n",
    "lst_of_docs = [\" \".join(doc.split())\n",
    "              for doc in lst_of_docs]\n",
    "print(lst_of_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d324c4-97aa-41a6-b205-ddc569271539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d0ebdcd-5ce3-43a7-b913-c9732a94e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', ': ', 'Hi', ', ', 'I ', 'am ', 'Christopher', 'Murillo', ', ', '\\n', 'I ', 'am ', 'experiencing ', 'a ', 'sharp ', 'pain ', 'in ', 'the ', 'lower ', 'right ', 'abdomen', ', ', 'occasionally ', 'radiating ', 'to ', 'my ', 'back', '. ', 'This ', 'symptom ', 'has ', 'been ', 'persistent ', 'for ', 'the ', 'past ', '4 ', 'days ', 'now', '. ', 'The ', 'pain ', 'is ', 'worsening ', 'over ', 'time', ', ', 'especially ', 'after ', 'eating ', 'or ', 'engaging ', 'in ', 'physical ', 'activities', '. ', 'I ', 'have ', 'noticed ', 'minor ', 'nausea', ', ', 'but ', 'no ', 'vomiting ', 'or ', 'bleeding', '. ', 'The ', 'pain ', 'is ', 'non', '-', 'specific', ', ', 'meaning ', 'it ', 'does', \"n't \", 'seem ', 'to ', 'be ', 'triggered ', 'by ', 'a ', 'specific ', 'food ', 'intake ', 'or ', 'any ', 'other ', 'activity', '. ', '\\n\\n', 'You ', 'can ', 'reach ', 'me ', 'at ', 'alvarezkenneth@gmail.com', 'or ', '493-290-9635', 'for ', 'any ', 'clarification ', 'or ', 'follow', '-', 'up ', 'questions', '.', '\\n\\n', 'I ', 'have ', 'a ', 'history ', 'of ', 'appendicitis ', 'which ', 'was ', 'surgically ', 'removed ', 'when ', 'I ', 'was ', '17', '. ', 'I ', 'have ', 'also ', 'had ', 'several ', 'episodes ', 'of ', 'stomach ', 'ulcers ', 'in ', 'the ', 'past', ', ', 'for ', 'which ', 'I ', 'have ', 'taken ', 'antacids ', 'and ', 'antibiotics ', 'as ', 'prescribed ', 'by ', 'my ', 'previous ', 'doctor', ', ', 'Dr. ', 'Emily ', 'Patel', ', ', 'whose ', 'contact ', 'information ', 'is ', 'available ', 'at ', 'https://facebook.com/healthcareprofessionals', '. ', '\\n\\n', 'Currently', ', ', 'I ', 'am ', 'not ', 'taking ', 'any ', 'medications', ', ', 'but ', 'I ', 'have ', 'an ', 'upcoming ', 'doctor', \"'s \", 'appointment ', 'with ', 'Dr. ', 'Sophia ', 'Lee ', 'on ', '6923421356', '. ', '\\n\\n', 'Can ', 'you ', 'please ', 'analyze ', 'my ', 'symptoms ', 'and ', 'help ', 'me ', 'understand ', 'what ', 'could ', 'be ', 'the ', 'underlying ', 'cause ', 'of ', 'this ', 'pain', '? ', 'What ', 'diagnostic ', 'tests ', 'would ', 'you ', 'recommend ', 'to ', 'confirm ', 'the ', 'diagnosis', '? ', 'Are ', 'there ', 'any ', 'lifestyle ', 'changes ', 'I ', 'can ', 'make ', 'to ', 'alleviate ', 'this ', 'discomfort', '? ', 'And ', 'finally', ', ', 'what ', 'are ', 'the ', 'treatment ', 'options ', 'available ', 'for ', 'this ', 'condition', '? ', 'I ', 'would ', 'appreciate ', 'it ', 'if ', 'you ', 'could ', 'send ', 'the ', 'response ', 'to ', 'my ', 'contact ', 'number ', '493-290-9635', 'for ', 'prompt ', 'action', '. ', '\\n\\n', 'I ', 'am ', 'willing ', 'to ', 'provide ', 'more ', 'information ', 'if ', 'required', '. ', 'You ', 'can ', 'also ', 'reach ', 'out ', 'to ', 'Dr. ', 'Michael ', 'Brown ', 'at ', 'michael.brown@sthealth.org ', 'or ', 'call ', 'him ', 'at ', '567', '-', '345', '-', '6789 ', 'for ', 'any ', 'additional ', 'information', '. ', 'My ', 'username ', 'is ', 'heathermartinez', ', ', 'and ', 'you ', 'can ', 'reach ', 'me ', 'anytime ', 'at ', '8223', 'Victoria', 'Row', 'or ', 'mail ', 'to ', 'St. ', 'Michael', \"'s \", 'Hospital', ', ', '1234 ', 'General ', 'Hospital ', 'Drive', ', ', 'Anytown', ', ', 'LA ', '70816', '. ', 'Thank ', 'you ', 'for ', 'your ', 'time ', 'and ', 'assistance', '.']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['medical_consultations']['tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a7385da-95af-4123-b8c9-cc257ba669fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset['medical_consultations'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f44b60b-812b-453b-9fca-d57ee80f88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['tokens'].apply(lambda x: \"\".join(x))\n",
    "dataset_txt=Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d817789-f3ee-4422-980e-e93ce61f0d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset_txt['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3925a520-1777-40bf-a05a-2d9a0dd9387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "#from transformers import pipeline\n",
    "\n",
    "#pipe = pipeline(\"token-classification\", model=\"obi/deid_roberta_i2b2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2bd71a3-1183-41e9-92b3-6dd42dd2269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results= pipe{dataset['medical_consultations']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28af1229-b997-4543-830b-b57ca62fa58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['medical_consultations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab825afe-2680-465a-87ef-c72ba27609a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results=[]\n",
    "#for out in pipe(dataset['medical_consultations']):\n",
    "    \n",
    "    #results.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3aea7e2-bbce-4c5c-8a0a-c9d91a6ba2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_v2 = dataset_txt['text'].map(\n",
    "          #  pipe,\n",
    "           # batched=True,\n",
    "            #batch_size=4,\n",
    "        # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f407bad-c975-4703-8776-e92d3112569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe(['my name is president Barack Obama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "570b09d0-d832-4f04-b284-ea4e5f7cab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results=[]\n",
    "# #with torch.inference_mode():\n",
    "# for inp in dataset_txt['text']:\n",
    "#     # print(inp)\n",
    "#     assert isinstance(inp, str), \"not a str\"\n",
    "#     out=pipe([inp], truncation=True)\n",
    "#     results.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78841311-a5e1-4684-8561-70c13fb25085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"obi/deid_roberta_i2b2\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"obi/deid_roberta_i2b2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6d7d50e-0f5c-4098-879d-8aa2c80b58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(inp, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fd1aef2-0ecc-47a1-8720-b86f6c54b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ac09766-2679-4820-acdc-78adb2237486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs.logits\n",
    "# tokenizer.decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab8d80c-1b2f-47aa-931e-c332dbc9e2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3795d80850594eb8bdd44c65cc1d7113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = pipe.tokenizer  # get tokenizer from your existing pipeline\n",
    "\n",
    "results = []\n",
    "for inp in tqdm(lst_of_docs):\n",
    "    assert isinstance(inp, str), \"not a str\"\n",
    "    \n",
    "    # Truncate the input at token level\n",
    "    inputs = tokenizer(\n",
    "        inp,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    # Decode back to string after truncation\n",
    "    truncated_text = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "    # Pass the truncated string to the pipeline\n",
    "    out = pipe(truncated_text)\n",
    "    results.append(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "14889b73-8471-453d-8155-d2fe9d1ee59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PATIENT', 'score': np.float32(0.99691075), 'index': 6, 'word': 'Jacqueline', 'start': 13, 'end': 23}, {'entity': 'L-PATIENT', 'score': np.float32(0.9775359), 'index': 7, 'word': 'Adams', 'start': 24, 'end': 29}, {'entity': 'U-STAFF', 'score': np.float32(0.7998271), 'index': 107, 'word': 'ch', 'start': 465, 'end': 467}, {'entity': 'U-PATIENT', 'score': np.float32(0.4773383), 'index': 187, 'word': 'mill', 'start': 790, 'end': 794}, {'entity': 'B-LOC', 'score': np.float32(0.9802167), 'index': 210, 'word': '67', 'start': 897, 'end': 899}, {'entity': 'B-LOC', 'score': np.float32(0.6470808), 'index': 211, 'word': '##7', 'start': 899, 'end': 900}, {'entity': 'I-LOC', 'score': np.float32(0.5506426), 'index': 212, 'word': '##1', 'start': 900, 'end': 901}, {'entity': 'I-LOC', 'score': np.float32(0.986521), 'index': 213, 'word': 'Johns', 'start': 902, 'end': 907}, {'entity': 'L-LOC', 'score': np.float32(0.7195891), 'index': 214, 'word': 'Shore', 'start': 908, 'end': 913}, {'entity': 'U-STAFF', 'score': np.float32(0.99487984), 'index': 303, 'word': 'her', 'start': 1366, 'end': 1369}, {'entity': 'U-STAFF', 'score': np.float32(0.97739923), 'index': 304, 'word': '##nan', 'start': 1369, 'end': 1372}, {'entity': 'U-STAFF', 'score': np.float32(0.9237844), 'index': 307, 'word': 'per', 'start': 1378, 'end': 1381}, {'entity': 'U-STAFF', 'score': np.float32(0.9790348), 'index': 312, 'word': 'list', 'start': 1391, 'end': 1395}, {'entity': 'B-LOC', 'score': np.float32(0.7664057), 'index': 326, 'word': '67', 'start': 1441, 'end': 1443}, {'entity': 'I-PHONE', 'score': np.float32(0.5780116), 'index': 328, 'word': '.', 'start': 1444, 'end': 1445}, {'entity': 'I-PHONE', 'score': np.float32(0.5422127), 'index': 329, 'word': '61', 'start': 1446, 'end': 1448}, {'entity': 'I-PHONE', 'score': np.float32(0.59821856), 'index': 330, 'word': '##6', 'start': 1448, 'end': 1449}, {'entity': 'I-PHONE', 'score': np.float32(0.72108173), 'index': 331, 'word': '.', 'start': 1449, 'end': 1450}, {'entity': 'L-PHONE', 'score': np.float32(0.89027214), 'index': 332, 'word': '34', 'start': 1451, 'end': 1453}, {'entity': 'L-PHONE', 'score': np.float32(0.7539926), 'index': 333, 'word': '##48', 'start': 1453, 'end': 1455}, {'entity': 'L-PHONE', 'score': np.float32(0.3941702), 'index': 334, 'word': '##x', 'start': 1455, 'end': 1456}, {'entity': 'L-PHONE', 'score': np.float32(0.28808916), 'index': 335, 'word': '##7', 'start': 1456, 'end': 1457}, {'entity': 'U-STAFF', 'score': np.float32(0.97552663), 'index': 362, 'word': 'Smith', 'start': 1559, 'end': 1564}, {'entity': 'U-PHONE', 'score': np.float32(0.99310076), 'index': 398, 'word': '55', 'start': 1726, 'end': 1728}, {'entity': 'U-PHONE', 'score': np.float32(0.6772597), 'index': 399, 'word': '##5', 'start': 1728, 'end': 1729}, {'entity': 'L-PHONE', 'score': np.float32(0.44468188), 'index': 401, 'word': '123', 'start': 1732, 'end': 1735}, {'entity': 'L-PHONE', 'score': np.float32(0.71347773), 'index': 403, 'word': '45', 'start': 1738, 'end': 1740}, {'entity': 'B-LOC', 'score': np.float32(0.9976165), 'index': 412, 'word': '123', 'start': 1771, 'end': 1774}, {'entity': 'I-LOC', 'score': np.float32(0.996711), 'index': 413, 'word': 'Main', 'start': 1775, 'end': 1779}, {'entity': 'L-LOC', 'score': np.float32(0.99892586), 'index': 414, 'word': 'St', 'start': 1780, 'end': 1782}, {'entity': 'U-LOC', 'score': np.float32(0.99708456), 'index': 416, 'word': 'Any', 'start': 1784, 'end': 1787}, {'entity': 'L-LOC', 'score': np.float32(0.98865324), 'index': 417, 'word': '##town', 'start': 1787, 'end': 1791}, {'entity': 'U-LOC', 'score': np.float32(0.99938965), 'index': 419, 'word': 'CA', 'start': 1793, 'end': 1795}, {'entity': 'U-LOC', 'score': np.float32(0.9993125), 'index': 420, 'word': '123', 'start': 1796, 'end': 1799}, {'entity': 'U-STAFF', 'score': np.float32(0.9983557), 'index': 432, 'word': 'Smith', 'start': 1846, 'end': 1851}, {'entity': 'B-STAFF', 'score': np.float32(0.7846137), 'index': 437, 'word': 'Emily', 'start': 1867, 'end': 1872}, {'entity': 'L-PATIENT', 'score': np.float32(0.7293447), 'index': 438, 'word': 'Chen', 'start': 1873, 'end': 1877}, {'entity': 'U-STAFF', 'score': np.float32(0.9827481), 'index': 443, 'word': 'ch', 'start': 1888, 'end': 1890}, {'entity': 'B-PATORG', 'score': np.float32(0.26081634), 'index': 446, 'word': 'dental', 'start': 1895, 'end': 1901}, {'entity': 'U-PHONE', 'score': np.float32(0.98377985), 'index': 453, 'word': '55', 'start': 1920, 'end': 1922}, {'entity': 'U-PHONE', 'score': np.float32(0.7040848), 'index': 454, 'word': '##5', 'start': 1922, 'end': 1923}, {'entity': 'I-PHONE', 'score': np.float32(0.29677773), 'index': 455, 'word': '-', 'start': 1924, 'end': 1925}, {'entity': 'L-PHONE', 'score': np.float32(0.81762034), 'index': 456, 'word': '234', 'start': 1926, 'end': 1929}, {'entity': 'I-PHONE', 'score': np.float32(0.40407184), 'index': 457, 'word': '-', 'start': 1930, 'end': 1931}, {'entity': 'L-PHONE', 'score': np.float32(0.9215562), 'index': 458, 'word': '56', 'start': 1932, 'end': 1934}]\n"
     ]
    }
   ],
   "source": [
    "print(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06433caa-8f48-421e-a258-85636035971d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d511c600-6b47-47e2-952a-59a25e718858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_entities(tokens, ner_tags):\n",
    "    \"\"\"\n",
    "    this function does something\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): labeled tokens\n",
    "        ner_tags (List[str]) Maybe ground truth labels? but idk\n",
    "\n",
    "    Returns:\n",
    "        entities (List[dict[str, str])): ????\n",
    "    \"\"\"\n",
    "    # label_map = dataset['medical_consultations'].features['ent_tags'].feature.names\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    for token, tag in zip(tokens, ner_tags):\n",
    "        # tag_label = label_map[tag]\n",
    "        if tag_label.startswith(\"B-\"):\n",
    "            if current_entity:\n",
    "                entities.append(\" \".join(current_entity))\n",
    "            current_entity = [token]\n",
    "        elif tag_label.startswith(\"I-\") and current_entity:\n",
    "            current_entity.append(token)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entities.append(\" \".join(current_entity))\n",
    "                current_entity = []\n",
    "    if current_entity:\n",
    "        entities.append(\" \".join(current_entity))\n",
    "    return entities\n",
    "\n",
    "# results3 = pipe(truncated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af6a131c-e5c3-409b-8e23-22e031d95cba",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Value' object has no attribute 'names'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m get_true_entities(results, lst_of_ent_tags)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mget_true_entities\u001b[39m\u001b[34m(tokens, ner_tags)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_true_entities\u001b[39m(tokens, ner_tags):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    this function does something\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03m        entities (List[dict[str, str])): ????\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     label_map = dataset[\u001b[33m'\u001b[39m\u001b[33mmedical_consultations\u001b[39m\u001b[33m'\u001b[39m].features[\u001b[33m'\u001b[39m\u001b[33ment_tags\u001b[39m\u001b[33m'\u001b[39m].feature.names\n\u001b[32m     13\u001b[39m     entities = []\n\u001b[32m     14\u001b[39m     current_entity = []\n",
      "\u001b[31mAttributeError\u001b[39m: 'Value' object has no attribute 'names'"
     ]
    }
   ],
   "source": [
    "get_true_entities(lst_of_ent_tags, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e27ea26-de7f-48ac-a753-1af4e30a6823",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m lst_of_lst_of_dict = copy.deepcopy(lst_of_ent_tags)\n\u001b[32m      2\u001b[39m tn,tp = \u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m\n\u001b[32m      3\u001b[39m fn,fp = \u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'copy' is not defined"
     ]
    }
   ],
   "source": [
    "lst_of_lst_of_dict = copy.deepcopy(lst_of_ent_tags)\n",
    "tn,tp = 0,0\n",
    "fn,fp = 0,0\n",
    "for i, lst in tqdm(enumerate(results)):\n",
    "    for dictionary in lst:\n",
    "        start_idx = dictionary['start']\n",
    "        end_idx = dictionary ['end']\n",
    "        doc_i = lst_of_docs[i]\n",
    "        tok = doc_i[start_idx:end_idx+1]\n",
    "        ent_tags = lst_of_ent_tags[i]\n",
    "        entity = dictionary['entity']\n",
    "        doc_tok = lst_of_tokens[i]\n",
    "        try:\n",
    "            \n",
    "            tok_idx = doc_tok.index(tok.strip())\n",
    "        except ValueError as ve:\n",
    "            fp += 1\n",
    "            continue\n",
    "# print(start_idx)\n",
    "# print(end_idx)\n",
    "# print(doc_i)\n",
    "# print(tok)\n",
    "# print(ent_tags)\n",
    "# print(entity)\n",
    "# print(doc_tok)\n",
    "# raise ValueError(f'{start_idx} oh no') from ve\n",
    "\n",
    "        try:\n",
    "            \n",
    "            tag = ent_tags[tok_idx]\n",
    "        except IndexError as ve:\n",
    "            print(doc_tok)\n",
    "            print(ent_tags)\n",
    "            assert len(doc_tok) == len(ent_tags), f'{len(doc_tok)} != {len(ent_tags)}'\n",
    "        is_pii = tag != 'O'\n",
    "        if is_pii: \n",
    "            tp += 0 \n",
    "        else:\n",
    "            \n",
    "            fp += 1\n",
    "        # print(tag)\n",
    "        # lst_of_lst_of_dict = \n",
    "        lst_of_lst_of_dict[i].remove(tag)\n",
    "    for tag in lst_of_lst_of_dict: \n",
    "        if tag == 'O':\n",
    "            \n",
    "            tnt = 1\n",
    "        else:\n",
    "             \n",
    "             fnt = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21711a04-66d7-4c01-a570-49645f513af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_of_lst_of_dict_entities = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stackleberggametheory",
   "language": "python",
   "name": "stackleberggametheory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
