{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617ab392-97a4-459e-bed8-d2147918e737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from dotenv import load_dotenv\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "pipe = pipeline(\"token-classification\", model=\"obi/deid_bert_i2b2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41926ff-e6f2-44e4-8c79-7aa9a4657370",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('healthcare_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e07d1b03-5a78-47a7-a254-7f90ff651e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mks-logic/SPY\", trust_remote_code=True, faker_random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d682e-d0b7-4088-a58f-3b564d84dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1312f34e-2b6d-4251-b99b-ffae978f9f0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2925930770.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdataset['medical_consultations'[.columns_names\u001b[39m\n                                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for x in dataset['medical_consultations']:\n",
    "    # print(x)\n",
    "    break\n",
    "dataset['medical_consultations'[.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7385da-95af-4123-b8c9-cc257ba669fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = dataset['medical_consultations'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f44b60b-812b-453b-9fca-d57ee80f88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text'] = df['tokens'].apply(lambda x: \"\".join(x))\n",
    "#dataset_txt=Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d817789-f3ee-4422-980e-e93ce61f0d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset_txt['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3925a520-1777-40bf-a05a-2d9a0dd9387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "#from transformers import pipeline\n",
    "\n",
    "#pipe = pipeline(\"token-classification\", model=\"obi/deid_roberta_i2b2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd71a3-1183-41e9-92b3-6dd42dd2269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results= pipe{dataset['medical_consultations']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af1229-b997-4543-830b-b57ca62fa58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['medical_consultations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab825afe-2680-465a-87ef-c72ba27609a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results=[]\n",
    "#for out in pipe(dataset['medical_consultations']):\n",
    "    \n",
    "    #results.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aea7e2-bbce-4c5c-8a0a-c9d91a6ba2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_v2 = dataset_txt['text'].map(\n",
    "          #  pipe,\n",
    "           # batched=True,\n",
    "            #batch_size=4,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f407bad-c975-4703-8776-e92d3112569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe(['my name is president Barack Obama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "570b09d0-d832-4f04-b284-ea4e5f7cab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results=[]\n",
    "# #with torch.inference_mode():\n",
    "# for inp in dataset_txt['text']:\n",
    "#     # print(inp)\n",
    "#     assert isinstance(inp, str), \"not a str\"\n",
    "#     out=pipe([inp], truncation=True)\n",
    "#     results.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78841311-a5e1-4684-8561-70c13fb25085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"obi/deid_roberta_i2b2\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"obi/deid_roberta_i2b2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7d50e-0f5c-4098-879d-8aa2c80b58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(inp, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1aef2-0ecc-47a1-8720-b86f6c54b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac09766-2679-4820-acdc-78adb2237486",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits\n",
    "# tokenizer.decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab8d80c-1b2f-47aa-931e-c332dbc9e2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c444f75c5ea4ce98a6b5aece9d4054d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = pipe.tokenizer  # get tokenizer from your existing pipeline\n",
    "\n",
    "results = []\n",
    "for inp in tqdm(dataset_txt['text']):\n",
    "    assert isinstance(inp, str), \"not a str\"\n",
    "    \n",
    "    # Truncate the input at token level\n",
    "    inputs = tokenizer(\n",
    "        inp,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    # Decode back to string after truncation\n",
    "    truncated_text = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "    # Pass the truncated string to the pipeline\n",
    "    out = pipe(truncated_text)\n",
    "    results.append(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d511c600-6b47-47e2-952a-59a25e718858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_entities(tokens, ner_tags):\n",
    "    \"\"\"\n",
    "    this function does something\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): labeled tokens\n",
    "        ner_tags (List[str]) Maybe ground truth labels? but idk\n",
    "\n",
    "    Returns:\n",
    "        entities (List[dict[str, str])): ????\n",
    "    \"\"\"\n",
    "    label_map = dataset['medical_consultations'].features['ner_tags'].feature.names\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    for token, tag in zip(tokens, ner_tags):\n",
    "        tag_label = label_map[tag]\n",
    "        if tag_label.startswith(\"B-\"):\n",
    "            if current_entity:\n",
    "                entities.append(\" \".join(current_entity))\n",
    "            current_entity = [token]\n",
    "        elif tag_label.startswith(\"I-\") and current_entity:\n",
    "            current_entity.append(token)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entities.append(\" \".join(current_entity))\n",
    "                current_entity = []\n",
    "    if current_entity:\n",
    "        entities.append(\" \".join(current_entity))\n",
    "    return entities\n",
    "\n",
    "results3 = pipe(truncated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af6a131c-e5c3-409b-8e23-22e031d95cba",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_true_entities() missing 1 required positional argument: 'ner_tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m get_true_entities(results[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mTypeError\u001b[39m: get_true_entities() missing 1 required positional argument: 'ner_tags'"
     ]
    }
   ],
   "source": [
    "get_true_entities(results[0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27ea26-de7f-48ac-a753-1af4e30a6823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stackleberggametheory",
   "language": "python",
   "name": "stackleberggametheory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
